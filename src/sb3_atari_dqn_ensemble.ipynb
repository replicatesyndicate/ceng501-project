{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "EXPERIMENT_NAME = \"sb3_atari_dqn_ensemble_1\"\n",
    "\n",
    "MODEL_PATH = f\"./models/{EXPERIMENT_NAME}\"\n",
    "LOG_PATH = f\"./logs/{EXPERIMENT_NAME}\"\n",
    "TENSORBOARD_LOG_PATH = f\"./logs/dqn_tensorboard_logs/atari/{EXPERIMENT_NAME}\"\n",
    "\n",
    "# Imports\n",
    "## numpy\n",
    "import numpy as np\n",
    "## pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "## stable-baselines3\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "# from stable_baselines3.common.env_util import make_atari_env # seems to be buggy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor # required for minigrid\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "from stable_baselines3.common.utils import polyak_update, get_linear_fn, set_random_seed\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.dqn.policies import CnnPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "## gymnasium\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "# from gymnasium.wrappers import FrameStackObservation, ClipReward\n",
    "## plotly and pyplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard setup\n",
    "writer = SummaryWriter(TENSORBOARD_LOG_PATH)\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_ENSEMBLE = 2  # Number of ensemble agents\n",
    "RESET_FREQUENCY = 40000  # Reset frequency in timesteps\n",
    "BETA = 50  # Action selection coefficient\n",
    "REPLAY_BUFFER_SIZE = 100000  # Replay buffer size\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "LEARNING_STARTS = 2000  # Timesteps before training starts\n",
    "TAU = 0.005  # Polyak update coefficient\n",
    "TOTAL_TIMESTEPS = int(1e5)  # Total training timesteps\n",
    "TRAIN_FREQ = 1  # Frequency of training (steps)\n",
    "GRADIENT_STEPS = 1  # Gradient steps per update\n",
    "TARGET_UPDATE_INTERVAL = 1  # Update target networks every step\n",
    "N_STACK = 4  # Number of stacked frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_freq = 5000 # once every eval_freq timesteps, evaluate the model\n",
    "# replay_ratio = 4 # run gradient calculations 4 times per step\n",
    "env_type = \"AlienNoFrameskip-v4\" # use this emulation from Gymnasium environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_type, rank=0, frameskip=1, render_mode=None, seed=0):\n",
    "    def _init():\n",
    "        env = gym.make(env_type, render_mode=render_mode, frameskip=frameskip) # frameskip is important\n",
    "        env = AtariWrapper(env)\n",
    "        env.action_space.seed(seed + rank)\n",
    "        env.observation_space.seed(seed + rank)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Observation Space: (4, 84, 84)\n",
      "Current Eval Observation Space: (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Create and preprocess Atari environment\n",
    "env = DummyVecEnv([make_env(env_type=env_type, seed=42)])\n",
    "env = VecTransposeImage(env)\n",
    "env = VecFrameStack(env, n_stack=N_STACK, channels_order='first')\n",
    "\n",
    "eval_env = DummyVecEnv([make_env(env_type=env_type, seed=84)])\n",
    "eval_env = VecTransposeImage(eval_env)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=N_STACK, channels_order='first')\n",
    "\n",
    "# Debug observation space       \n",
    "print(f\"Current Observation Space: {env.observation_space.shape}\")\n",
    "print(f\"Current Eval Observation Space: {eval_env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare an feature extractor capable of overseeing n_stack frames\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "# Prepare custom policy definition to fit into ensemble\n",
    "class CustomQPolicy(BasePolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        lr_schedule,\n",
    "        net_arch=None,\n",
    "        features_dim=512,\n",
    "        activation_fn=nn.ReLU,\n",
    "        ortho_init=True,\n",
    "        device=\"auto\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            ortho_init,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        self.features_extractor = CustomCNN(self.observation_space, features_dim=512)\n",
    "        self.q_net = nn.Linear(self.features_extractor.features_dim, action_space.n)\n",
    "        self.q_net = self.q_net.to(self.device)\n",
    "\n",
    "    def forward(self, obs, deterministic=True):\n",
    "      return self.q_net(self.extract_features(obs))\n",
    "\n",
    "    def _predict(self, observation, deterministic=True):\n",
    "        return self(observation)\n",
    "\n",
    "    def extract_features(self, obs):\n",
    "        return self.features_extractor(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE, env.observation_space, env.action_space, device=device, n_envs=4)\n",
    "\n",
    "# # Ensemble setup\n",
    "ensemble_agents = [\n",
    "    CustomQPolicy(\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        lr_schedule=get_linear_fn(1e-4, 1e-5, 1.0),\n",
    "        features_dim=336,\n",
    "    ).to(device)\n",
    "    for _ in range(N_ENSEMBLE)\n",
    "]\n",
    "target_networks = [\n",
    "    CustomQPolicy(\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        lr_schedule=get_linear_fn(1e-4, 1e-5, 1.0),\n",
    "        features_dim=336,\n",
    "    ).to(device)\n",
    "    for _ in range(N_ENSEMBLE)\n",
    "]\n",
    "# ensemble_agents = [\n",
    "#     CnnPolicy(\n",
    "#         observation_space=env.observation_space,\n",
    "#         action_space=env.action_space,\n",
    "#         feature_extractor_class=CustomCNN(observation_space=env.observation_space, n_stack=N_STACK),\n",
    "#         lr_schedule=get_linear_fn(1e-4, 1e-5, 1.0),\n",
    "#         # net_arch=[256, 256], # do not use along with custom CNN definition\n",
    "#     ).to(device)\n",
    "#     for _ in range(N_ENSEMBLE)\n",
    "# ]\n",
    "# target_networks = [\n",
    "#     CnnPolicy(\n",
    "#         observation_space=env.observation_space,\n",
    "#         action_space=env.action_space,\n",
    "#         feature_extractor_class=CustomCNN(observation_space=env.observation_space, n_stack=N_STACK),\n",
    "#         lr_schedule=get_linear_fn(1e-4, 1e-5, 1.0),\n",
    "#         # net_arch=[256, 256], # do not use along with custom CNN definition\n",
    "#     ).to(device)\n",
    "#     for _ in range(N_ENSEMBLE)\n",
    "# ]\n",
    "optimizers = [optim.Adam(agent.parameters(), lr=1e-4) for agent in ensemble_agents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callback Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_agent(agent):\n",
    "    for layer in agent.q_net.modules():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def adaptive_action_selection(q_values, beta):\n",
    "    q_values_normalized = q_values / (q_values.max(dim=-1, keepdim=True)[0] + 1e-8)\n",
    "    scaled_q_values = beta * torch.max(q_values_normalized, torch.zeros_like(q_values_normalized))\n",
    "    summed_q_values = scaled_q_values.sum(dim=0)\n",
    "    action_distributions = torch.softmax(summed_q_values, dim=-1)\n",
    "    print(action_distributions.shape)\n",
    "    # Vectorized sampling\n",
    "    actions = torch.multinomial(action_distributions, num_samples=4).squeeze(-1).cpu().numpy()\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using DQN implementation of Stable-Baselines3 with modified callbacks\n",
    "\n",
    "# # callback frequencies are scaled to stack counts to match the given actual game timestep\n",
    "# eval_callback = EvalCallback(env, best_model_save_path=LOG_PATH, log_path=LOG_PATH,\n",
    "#                              eval_freq=max(eval_freq // n_stack, 1), deterministic=True,\n",
    "#                              render=True)\n",
    "# # Create and attach the callback\n",
    "# reset_callback = ResetWeightsCallback(reset_interval=max(reset_interval // n_stack, 1), verbose=1)\n",
    "\n",
    "# callback_list = CallbackList([eval_callback, reset_callback])\n",
    "\n",
    "# model = DQN(\n",
    "#     policy= \"CnnPolicy\", \n",
    "#     env= env, \n",
    "#     verbose= 1, \n",
    "#     buffer_size= timesteps,\n",
    "#     learning_starts= 2000,\n",
    "#     tau= 0.005,\n",
    "#     train_freq= (1, \"step\"),\n",
    "#     gradient_steps= replay_ratio,\n",
    "#     target_update_interval= 1,\n",
    "#     policy_kwargs= policy_kwargs,\n",
    "#     tensorboard_log=\"./dqn_tensorboard_logs/atari\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: (1, 4, 84, 84)\n",
      "state tensor shape: torch.Size([1, 4, 84, 84])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (336x84 and 512x18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mensemble_agents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m q_values_stack \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([agent\u001b[38;5;241m.\u001b[39mq_net(state_tensor) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m ensemble_agents])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_values_stack shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_values_stack\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (336x84 and 512x18)"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "state = env.reset()\n",
    "print(f\"state shape: {state.shape}\")\n",
    "current_agent_index = 0\n",
    "step_count = 0\n",
    "\n",
    "for step in range(TOTAL_TIMESTEPS):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    print(f\"state tensor shape: {state_tensor.shape}\")\n",
    "    ensemble_agents[0].q_net(state_tensor)\n",
    "    q_values_stack = torch.stack([agent.q_net(state_tensor) for agent in ensemble_agents])\n",
    "    print(f\"q_values_stack shape: {q_values_stack.shape}\")\n",
    "\n",
    "    actions = np.array([adaptive_action_selection(q_values, BETA) for q_values in q_values_stack])\n",
    "    next_state, reward, done, info = env.step(actions)\n",
    "\n",
    "    replay_buffer.add(state, next_state, actions, reward, done, info)\n",
    "    state = next_state\n",
    "\n",
    "    if replay_buffer.size() > BATCH_SIZE and step > LEARNING_STARTS:\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        observations = batch.observations.to(device)\n",
    "        next_observations = batch.next_observations.to(device)\n",
    "\n",
    "        for i, agent in enumerate(ensemble_agents):\n",
    "            q_values = agent.q_net(observations).gather(1, batch.actions.to(device).long())\n",
    "            with torch.no_grad():\n",
    "                target_q_values = target_networks[i].q_net(next_observations).max(1, keepdim=True)[0]\n",
    "                target = batch.rewards.to(device) + GAMMA * (1 - batch.dones.to(device)) * target_q_values\n",
    "\n",
    "            loss = F.smooth_l1_loss(q_values, target)\n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "\n",
    "    if step % TARGET_UPDATE_INTERVAL == 0:\n",
    "        for i in range(N_ENSEMBLE):\n",
    "            polyak_update(ensemble_agents[i].q_net.parameters(), target_networks[i].q_net.parameters(), TAU)\n",
    "\n",
    "    if step % RESET_FREQUENCY == 0:\n",
    "        reset_agent(ensemble_agents[current_agent_index])\n",
    "        current_agent_index = (current_agent_index + 1) % N_ENSEMBLE\n",
    "\n",
    "    if np.any(done):\n",
    "        state = env.reset()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}, Average Reward: {np.mean(reward)}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.125"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "def evaluate(agents, env, n_eval_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = np.zeros(env.num_envs)\n",
    "        done = np.zeros(env.num_envs, dtype=bool)\n",
    "\n",
    "        while not np.all(done):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            q_values = torch.stack([agent.q_net(state_tensor) for agent in agents])\n",
    "            action = torch.argmax(q_values.mean(dim=0), dim=1).cpu().numpy()\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(np.mean(episode_reward))\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Evaluation Results: Mean Reward = {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "# Perform evaluation\n",
    "evaluate(ensemble_agents, eval_env)\n",
    "\n",
    "# Close environments\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
