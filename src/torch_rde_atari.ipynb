{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CENG501 Final Project - Reset Deep Ensembles\n",
    "Authors: Ege Uğur Aguş and Atakan Botasun </br>\n",
    "\n",
    "A Reset Deep Ensemble (RDE) script for the Atari-100k benchmark, using the hyperparameters from the paper's Section 4 / Appendix B.\n",
    "\n",
    "Specifically for AlienNoFrameskip-v4: </br>\n",
    "- 100k environment steps total\n",
    "- Reset interval: 8e4 (80,000)\n",
    "- Reset depth: \"last1\"\n",
    "- Replay buffer size: 1e5\n",
    "- Min replay size: 1e4\n",
    "- Batch size: 32\n",
    "- Target net update period: 1 (i.e., every training update)\n",
    "- Max gradient norm: 10\n",
    "- Softmax β = 50\n",
    "- Possibly do 4 updates per environment step (replay ratio = 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "# Register the Atari environments (ALE) and Minigrid\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Hyperparameters (referenced from the paper)\n",
    "ENV_ID              = \"Freeway-v4\"\n",
    "N_ENSEMBLE          = 2           # # of ensemble agents\n",
    "N_ENVS              = 1           # Single env to can be used to keep it truly at N time steps\n",
    "LR                  = 1e-4        # Learning rate\n",
    "GAMMA               = 0.99        # Discount factor\n",
    "BATCH_SIZE          = 32          # Batch size\n",
    "REPLAY_BUFFER_SIZE  = 10_000     # Replay buffer size (paper uses 100 000) - reduced to 10% of original for performance\n",
    "MIN_REPLAY_SIZE     = 1_000      # Mininum replay buffer size (paper uses 10 000) - reduced to 10% of original for performance\n",
    "TOTAL_TIMESTEPS     = 100_000     # Total time steps\n",
    "TRAIN_FREQUENCY     = 1           # train every environment step\n",
    "UPDATES_PER_STEP    = 1           # replay ratio (1,2,4) => pick 4\n",
    "TARGET_UPDATE_FREQ  = 1           # update target net every training step\n",
    "\n",
    "RESET_FREQUENCY     = 40_000      # 8e4 for Alien, Pong, 4e4 for Freeway\n",
    "RESET_DEPTH         = \"last1\"     # \"last1\" for Alien\n",
    "                                  # \"last2\" for Pong\n",
    "SOFTMAX_BETA        = 50          # beta for action selection\n",
    "\n",
    "EPS_START           = 1.0\n",
    "EPS_END             = 0.01\n",
    "EPS_DECAY_FRAC      = 0.2         # decay epsilon over 0.01% of total steps => 10k\n",
    "\n",
    "MAX_GRAD_NORM       = 10          # paper indicates max grad norm = 10\n",
    "\n",
    "# View performance during runs\n",
    "LOG_INTERVAL        = 1_000\n",
    "RENDER_INTERVAL     = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedFrames(gym.Wrapper):\n",
    "    def __init__(self, env, n_stack=4, channels_last=True):\n",
    "        super().__init__(env)\n",
    "        self.n_stack = n_stack\n",
    "        self.channels_last = channels_last\n",
    "        self.stack = None\n",
    "        obs_shape = env.observation_space.shape\n",
    "        print(obs_shape)\n",
    "        if channels_last:\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0, high=255,\n",
    "                shape=(obs_shape[0], obs_shape[1], n_stack),\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "        else:  # Channels first\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0, high=255,\n",
    "                shape=(obs_shape[0] * n_stack, obs_shape[1], 1),\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        if self.channels_last:\n",
    "            self.stack = np.repeat(obs[..., np.newaxis], self.n_stack, axis=-1)\n",
    "        else:\n",
    "            self.stack = np.repeat(obs[np.newaxis, ...], self.n_stack, axis=0)\n",
    "        return self.stack, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if self.channels_last:\n",
    "            self.stack = np.roll(self.stack, shift=-obs.shape[-1], axis=-1)\n",
    "            self.stack[..., -obs.shape[-1]:] = obs\n",
    "        else:\n",
    "            self.stack = np.roll(self.stack, shift=-obs.shape[0], axis=0)\n",
    "            self.stack[-obs.shape[0]:, ...] = obs\n",
    "        return self.stack, reward, terminated, truncated, info\n",
    "\n",
    "# Build multiple VecEnv with StackedFrames\n",
    "def make_vec_env_gym(env_id, n_envs=1, seed=0, channels_last=True):\n",
    "    def _make_env():\n",
    "        env = gym.make(env_id, render_mode=None) # Important for vectorization\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        if isinstance(env.action_space, gym.spaces.Box):\n",
    "            env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        if not channels_last:\n",
    "            obs_shape = env.observation_space.shape\n",
    "            print(obs_shape)\n",
    "            new_obs_shape = (obs_shape[2], obs_shape[0], obs_shape[1])\n",
    "            new_observation_space = gym.spaces.Box(low=0, high=1, shape=new_obs_shape, dtype=np.float32)\n",
    "            env = gym.wrappers.TransformObservation(env, lambda obs: np.transpose(obs, (2, 0, 1)), observation_space=new_observation_space)\n",
    "            env = gym.wrappers.TransformObservation(env, lambda obs: np.transpose(obs, (2, 0, 1)) if channels_last==False else obs, new_observation_space) # Channels first if needed\n",
    "        return env\n",
    "\n",
    "    if n_envs > 1:\n",
    "      envs = gym.vector.VectorEnv([_make_env for _ in range(n_envs)])\n",
    "      envs = gym.wrappers.VecFrameStack(envs, n_stack=4)\n",
    "      return envs\n",
    "\n",
    "    else:\n",
    "      env = _make_env()\n",
    "      env = StackedFrames(env, n_stack=4, channels_last=channels_last)\n",
    "      return env\n",
    "    \n",
    "# Build single VecEnv with Stable-Baselines 3 implementation\n",
    "def make_vec_env_sb3(env_id=ENV_ID, n_envs=N_ENVS, seed=0):\n",
    "    # Gray-scaling, 84x84, 4 frame stacks, reward clipping, etc. are typical\n",
    "    # for \"make_atari_env\". We assume that or we can provide wrapper_kwargs\n",
    "    venv = make_atari_env(env_id, n_envs=n_envs, seed=seed)\n",
    "    # Frame stacking => shape: (n_envs, 84,84,4)\n",
    "    venv = VecFrameStack(venv, n_stack=4)\n",
    "    venv.current_obs = None\n",
    "    return venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   CNN Q-Network for Atari (3 conv layers + 1 FC)\n",
    "class QNetworkAtari(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        # From paper: 3 conv layers [32,64,64], kernels [8x8,4x4,3x3], strides [4,2,1].\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 7x7 is typical after [8,4,3] filters/strides\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        self.n_actions = n_actions\n",
    "        self.reset_parameters(\"full\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, 4, 84, 84]\n",
    "        feats = self.features(x)\n",
    "        feats = feats.contiguous().view(feats.size(0), -1)\n",
    "        out = self.fc(feats)\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self, reset_depth=\"full\"):\n",
    "        \"\"\"Reset some or all of the network parameters.\"\"\"\n",
    "        def _init_module(m):\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        if reset_depth == \"full\":\n",
    "            self.apply(_init_module)\n",
    "        elif reset_depth == \"last2\":\n",
    "            # Re-init the last 2 layers in self.fc\n",
    "            if len(self.fc) == 3:\n",
    "                _init_module(self.fc[-1])\n",
    "                _init_module(self.fc[-3])\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected architecture for partial reset.\")\n",
    "        elif reset_depth == \"last1\":\n",
    "            # Re-init ONLY the final linear layer in self.fc\n",
    "            if len(self.fc) == 3:\n",
    "                _init_module(self.fc[-1])\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected architecture for partial reset.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unknown reset depth option.\")\n",
    "\n",
    "class QNetworkMiniGrid(nn.Module):\n",
    "    def _init_(self, obs_dim, n_actions, hidden_size = 256):\n",
    "        super()._init_()\n",
    "        # We'll define a 5-layer MLP (including output).\n",
    "        # For example: input -> 256 -> 256 -> 256 -> 256 -> output(n_actions)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "        # If we want partial resets, we can define \"reset_parameters\" similarly.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, obs_dim]\n",
    "        return self.net(x)\n",
    "\n",
    "#   Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        self.buffer.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "    def sample(self, batch_size=32):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, acts, rews, next_obs, dones = zip(*batch)\n",
    "\n",
    "        obs       = np.stack(obs)        # shape: [B, 4,84,84]\n",
    "        acts      = np.array(acts, dtype=np.int64)\n",
    "        rews      = np.array(rews, dtype=np.float32)\n",
    "        next_obs  = np.stack(next_obs)\n",
    "        dones     = np.array(dones, dtype=np.float32)\n",
    "        return obs, acts, rews, next_obs, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity, obs_shape, action_dim, device=\"cpu\"):\n",
    "#         self.capacity = capacity\n",
    "#         self.device = device\n",
    "#         self.obs = torch.zeros((capacity,) + obs_shape, dtype=torch.float32, device=self.device)\n",
    "#         self.next_obs = torch.zeros((capacity,) + obs_shape, dtype=torch.float32, device=self.device)\n",
    "#         self.actions = torch.zeros(capacity, dtype=torch.int64, device=self.device)\n",
    "#         self.rewards = torch.zeros(capacity, dtype=torch.float32, device=self.device)\n",
    "#         self.dones = torch.zeros(capacity, dtype=torch.float32, device=self.device)  # Store as float for easier use in loss functions\n",
    "#         self.ptr = 0\n",
    "#         self.size = 0\n",
    "\n",
    "#     def add(self, obs, action, reward, next_obs, done):\n",
    "#         self.obs[self.ptr] = torch.tensor(obs, dtype=torch.float32, device=self.device) if not torch.is_tensor(obs) else obs.to(self.device)\n",
    "#         self.next_obs[self.ptr] = torch.tensor(next_obs, dtype=torch.float32, device=self.device) if not torch.is_tensor(next_obs) else next_obs.to(self.device)\n",
    "#         self.actions[self.ptr] = action if torch.is_tensor(action) else torch.tensor(action, dtype=torch.int64, device=self.device)\n",
    "#         self.rewards[self.ptr] = reward if torch.is_tensor(reward) else torch.tensor(reward, dtype=torch.float32, device=self.device)\n",
    "#         self.dones[self.ptr] = float(done) if not torch.is_tensor(done) else done.to(self.device) # Ensure it's a float\n",
    "\n",
    "#         self.ptr = (self.ptr + 1) % self.capacity\n",
    "#         self.size = min(self.size + 1, self.capacity)\n",
    "#         return self.obs, self.actions, self.rewards, self.next_obs, self.dones\n",
    "\n",
    "#     def sample(self, batch_size=32):\n",
    "#         indices = torch.randint(0, self.size, (batch_size,), device=self.device)\n",
    "#         return (\n",
    "#             self.obs[indices],\n",
    "#             self.actions[indices],\n",
    "#             self.rewards[indices].unsqueeze(1),  # Add dimension for rewards\n",
    "#             self.next_obs[indices],\n",
    "#             self.dones[indices].unsqueeze(1),    # Add dimension for dones\n",
    "#         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#   Ensemble DQN Agent\n",
    "class EnsembleDQNAgent:\n",
    "    def __init__(self, n_actions,\n",
    "                 n_ensemble=N_ENSEMBLE, lr=LR, gamma=GAMMA,\n",
    "                 reset_freq=RESET_FREQUENCY, reset_depth=RESET_DEPTH,\n",
    "                 softmax_beta=SOFTMAX_BETA, feature_type=\"atari\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", self.device)\n",
    "\n",
    "        self.n_ensemble = n_ensemble\n",
    "        self.gamma = gamma\n",
    "        self.reset_freq = reset_freq\n",
    "        self.reset_depth = reset_depth\n",
    "        self.softmax_beta = softmax_beta\n",
    "\n",
    "        self.q_networks = []\n",
    "        self.target_networks = []\n",
    "        self.optimizers = []\n",
    "\n",
    "        for _ in range(n_ensemble):\n",
    "            qnet = QNetworkAtari(n_actions).to(self.device)\n",
    "            tnet = QNetworkAtari(n_actions).to(self.device)\n",
    "            tnet.load_state_dict(qnet.state_dict())\n",
    "\n",
    "            optimizer = optim.Adam(qnet.parameters(), lr=lr)\n",
    "            self.q_networks.append(qnet)\n",
    "            self.target_networks.append(tnet)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "        self.global_step = 0\n",
    "\n",
    "        # Round-robin\n",
    "        self.last_reset_idx = 0\n",
    "        # The 'oldest' agent is next after the just-reset agent\n",
    "        self.oldest_agent_idx = (self.last_reset_idx + 1) % self.n_ensemble\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def select_action(self, obs_np, epsilon=0.05):\n",
    "        # Epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "\n",
    "        # obs_np shape: (84,84,4) => transpose to (4,84,84)\n",
    "        obs_ch_first = np.transpose(obs_np, (2,0,1))\n",
    "        obs_t = torch.from_numpy(obs_ch_first).unsqueeze(0).float().to(self.device)\n",
    "\n",
    "        # Each agent picks argmax\n",
    "        candidate_actions = []\n",
    "        with torch.no_grad():\n",
    "            for qnet in self.q_networks:\n",
    "                qvals = qnet(obs_t)\n",
    "                a = qvals.argmax(dim=1).item()\n",
    "                candidate_actions.append(a)\n",
    "\n",
    "        # Use oldest agent's Q-values to compute softmax distribution\n",
    "        oldest = self.oldest_agent_idx\n",
    "        with torch.no_grad():\n",
    "            qvals_oldest = self.q_networks[oldest](obs_t).squeeze(0)\n",
    "\n",
    "        # For each agent's chosen action, get Q_oldest(s, a_i)\n",
    "        r_values = []\n",
    "        for act in candidate_actions:\n",
    "            r_values.append(qvals_oldest[act].item())\n",
    "\n",
    "        # Scale for stable softmax\n",
    "        max_r = max(abs(v) for v in r_values) if r_values else 1.0\n",
    "        if max_r == 0:\n",
    "            max_r = 1.0\n",
    "        scaled_r = [(val / max_r) * self.softmax_beta for val in r_values]\n",
    "        exp_r = np.exp(scaled_r)\n",
    "        sum_exp = np.sum(exp_r)\n",
    "        if sum_exp <= 1e-9:\n",
    "            probs = np.ones(self.n_ensemble) / self.n_ensemble\n",
    "        else:\n",
    "            probs = exp_r / sum_exp\n",
    "\n",
    "        chosen_agent_idx = np.random.choice(self.n_ensemble, p=probs)\n",
    "        return candidate_actions[chosen_agent_idx]\n",
    "\n",
    "    def reset_agent(self, idx):\n",
    "        \"\"\"Reset the parameters for agent idx.\"\"\"\n",
    "        self.q_networks[idx].reset_parameters(self.reset_depth)\n",
    "        self.target_networks[idx].load_state_dict(\n",
    "            self.q_networks[idx].state_dict()\n",
    "        )\n",
    "\n",
    "    def step_env(self, vec_env, replay_buffer, obs_np=None, epsilon=0.05):\n",
    "        \"\"\"\n",
    "        Interact with the single-env (n_envs=1) or multi-env. We store transitions.\n",
    "        \"\"\"\n",
    "        if isinstance(vec_env, VecFrameStack):\n",
    "            obs_np = vec_env.current_obs  # shape: (N_ENVS, 84,84,4)\n",
    "\n",
    "        n_envs = obs_np.shape[0]\n",
    "\n",
    "        # Get actions from the ensemble\n",
    "        actions = []\n",
    "        for i in range(n_envs):\n",
    "            single_obs = obs_np[i]  # (84,84,4)\n",
    "            a = self.select_action(single_obs, epsilon=epsilon)\n",
    "            actions.append(a)\n",
    "\n",
    "        next_obs, rewards, dones, infos = vec_env.step(actions)\n",
    "        self.global_step += n_envs\n",
    "\n",
    "        # Store transitions\n",
    "        for i in range(n_envs):\n",
    "            single_obs    = obs_np[i]\n",
    "            single_next   = next_obs[i]\n",
    "            single_reward = rewards[i]\n",
    "            done_bool     = bool(dones[i])\n",
    "\n",
    "            # Transpose to channels-first before storing\n",
    "            obs_ch_first     = np.transpose(single_obs,  (2,0,1))\n",
    "            next_ch_first    = np.transpose(single_next, (2,0,1))\n",
    "\n",
    "            replay_buffer.add(obs_ch_first, actions[i],\n",
    "                              single_reward, next_ch_first,\n",
    "                              done_bool)\n",
    "\n",
    "        # Round-robin reset\n",
    "        if (self.global_step % self.reset_freq) == 0:\n",
    "            print(f\"[INFO] Resetting agent {self.last_reset_idx} at step {self.global_step}\")\n",
    "            self.reset_agent(self.last_reset_idx)\n",
    "            self.oldest_agent_idx = (self.last_reset_idx + 1) % self.n_ensemble\n",
    "            self.last_reset_idx = (self.last_reset_idx + 1) % self.n_ensemble\n",
    "\n",
    "        # Return aggregated reward, done if any env ended\n",
    "        return float(np.mean(rewards)), any(dones), next_obs, actions\n",
    "\n",
    "    def train_on_batch(self, replay_buffer):\n",
    "        if len(replay_buffer) < MIN_REPLAY_SIZE:\n",
    "            return\n",
    "        obs, acts, rews, next_obs, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        obs_t      = torch.FloatTensor(obs).to(self.device)\n",
    "        acts_t     = torch.LongTensor(acts).unsqueeze(1).to(self.device)\n",
    "        rews_t     = torch.FloatTensor(rews).unsqueeze(1).to(self.device)\n",
    "        next_obs_t = torch.FloatTensor(next_obs).to(self.device)\n",
    "        dones_t    = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        for i in range(self.n_ensemble):\n",
    "            with torch.no_grad():\n",
    "                q_next = self.target_networks[i](next_obs_t)\n",
    "                max_q_next = q_next.max(dim=1, keepdim=True)[0]\n",
    "                target = rews_t + self.gamma * (1 - dones_t) * max_q_next\n",
    "\n",
    "            current_q = self.q_networks[i](obs_t).gather(1, acts_t)\n",
    "            # loss = nn.SmoothL1Loss()(current_q, target)\n",
    "            loss = nn.HuberLoss()(current_q, target)\n",
    "\n",
    "            self.optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients per the paper (max grad norm=10)\n",
    "            nn.utils.clip_grad_norm_(self.q_networks[i].parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "            self.optimizers[i].step()\n",
    "\n",
    "    # Tensor version\n",
    "    # def train_on_batch(self, replay_buffer):\n",
    "    #     if len(replay_buffer) < MIN_REPLAY_SIZE:\n",
    "    #         return\n",
    "\n",
    "    #     obs_t, acts_t, rews_t, next_obs_t, dones_t = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    #     for i in range(self.n_ensemble):\n",
    "    #         with torch.no_grad():\n",
    "    #             q_next = self.target_networks[i](next_obs_t)\n",
    "    #             max_q_next = q_next.max(dim=1, keepdim=True)[0]\n",
    "    #             target = rews_t + self.gamma * (1 - dones_t) * max_q_next\n",
    "\n",
    "    #         current_q = self.q_networks[i](obs_t).gather(1, acts_t)\n",
    "    #         loss = nn.HuberLoss()(current_q, target)\n",
    "\n",
    "    #         self.optimizers[i].zero_grad()\n",
    "    #         loss.backward()\n",
    "\n",
    "    #         nn.utils.clip_grad_norm_(self.q_networks[i].parameters(), MAX_GRAD_NORM)\n",
    "    #         self.optimizers[i].step()\n",
    "            \n",
    "    def update_target_nets(self):\n",
    "        \"\"\"Here, we copy the online Q-net parameters into each target net.\"\"\"\n",
    "        for i in range(self.n_ensemble):\n",
    "            self.target_networks[i].load_state_dict(\n",
    "                self.q_networks[i].state_dict()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "def run_training(env: Union[StackedFrames, VecFrameStack], render_env: Union[StackedFrames, VecFrameStack], verbose: int=0):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    agent = EnsembleDQNAgent(n_actions=n_actions)\n",
    "\n",
    "    obs_np = env.reset()  # shape: (N_ENVS, 84,84,4)\n",
    "    render_obs = render_env.reset()\n",
    "    env.current_obs = obs_np\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"Observation space size: {env.observation_space.shape}\")\n",
    "\n",
    "    # replay_buffer = ReplayBuffer(obs_shape=env.observation_space.shape[::-1], action_dim=n_actions,capacity=REPLAY_BUFFER_SIZE, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    replay_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_SIZE)\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_count = 0\n",
    "    rewards_history = []\n",
    "\n",
    "    train_steps = 0\n",
    "    # We'll decay epsilon over the first 10% (EPS_DECAY_FRAC=0.1) of total timesteps => 10k steps\n",
    "    eps_decay_steps = int(EPS_DECAY_FRAC * TOTAL_TIMESTEPS)\n",
    "\n",
    "    print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps on {ENV_ID} with RDE...\")\n",
    "\n",
    "    # Timers\n",
    "    iteration_start = time.monotonic()\n",
    "    action_log = []\n",
    "\n",
    "    while agent.global_step < TOTAL_TIMESTEPS:\n",
    "        \n",
    "        fraction = min(1.0, agent.global_step / eps_decay_steps)  # in [0,1]\n",
    "        epsilon = EPS_START + fraction * (EPS_END - EPS_START)\n",
    "        epsilon = max(EPS_END, epsilon)  # clamp\n",
    "\n",
    "        r, done, obs_np, actions = agent.step_env(env, replay_buffer, obs_np=obs_np, epsilon=epsilon)\n",
    "               \n",
    "        episode_reward += r\n",
    "\n",
    "        if done:\n",
    "            episode_count += 1\n",
    "            rewards_history.append(episode_reward)\n",
    "            obs = env.reset()\n",
    "            render_obs = render_env.reset()\n",
    "            env.current_obs = obs\n",
    "            episode_reward = 0.0\n",
    "\n",
    "        # train multiple times => replay ratio=4\n",
    "        if (agent.global_step % TRAIN_FREQUENCY) == 0:\n",
    "            for _ in range(UPDATES_PER_STEP):\n",
    "                agent.train_on_batch(replay_buffer)\n",
    "                train_steps += 1\n",
    "\n",
    "        # Because the table says \"target update period=1\",\n",
    "        # we do it every training step\n",
    "        if train_steps > 0 and (train_steps % TARGET_UPDATE_FREQ) == 0:\n",
    "            agent.update_target_nets()\n",
    "\n",
    "        # Rendering (just sample one of the actions from all environments)\n",
    "        _ = render_env.step(actions[0])\n",
    "        if (agent.global_step % RENDER_INTERVAL) == 0 and (agent.global_step > TOTAL_TIMESTEPS // 2):\n",
    "            render_env.render()\n",
    "\n",
    "        action_log.append(actions[0])\n",
    "\n",
    "        # Logging\n",
    "        if (agent.global_step % LOG_INTERVAL) == 0 and agent.global_step > 0:\n",
    "            iteration_end = time.monotonic()\n",
    "            last_10 = np.mean(rewards_history[-10:]) if len(rewards_history)>=10 else np.mean(rewards_history)\n",
    "            elapsed = iteration_start-iteration_end\n",
    "            print(f\"Step={agent.global_step} | Episodes={episode_count} | \"\n",
    "                  f\"AvgRew(last10)={last_10:.2f} | Eps={epsilon:.3f} | Real Time Elapsed={(time.time()-elapsed):.2f}s\")\n",
    "            print(f\"Last taken actions: {action_log[-10:]}\")\n",
    "\n",
    "            iteration_start = time.monotonic()\n",
    "        \n",
    "    final_10 = np.mean(rewards_history[-10:]) if len(rewards_history)>=10 else 0.0\n",
    "    total_elapsed = time.monotonic() - start_time\n",
    "    print(f\"Finished. Total elapsed time: {total_elapsed:.2f}s\")\n",
    "\n",
    "    env.close()\n",
    "    render_env.close()\n",
    "    return final_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Observation space size: (84, 84, 4)\n",
      "Starting training for 100000 timesteps on Freeway-v4 with RDE...\n",
      "Step=1000 | Episodes=1 | AvgRew(last10)=0.00 | Eps=0.975 | Real Time Elapsed=1736538661.11s\n",
      "Last taken actions: [1, 0, 1, 1, 1, 2, 2, 0, 1, 0]\n",
      "Step=2000 | Episodes=2 | AvgRew(last10)=0.50 | Eps=0.951 | Real Time Elapsed=1736538710.68s\n",
      "Last taken actions: [0, 1, 2, 1, 1, 0, 1, 1, 0, 0]\n",
      "Step=3000 | Episodes=4 | AvgRew(last10)=0.25 | Eps=0.926 | Real Time Elapsed=1736538760.58s\n",
      "Last taken actions: [1, 2, 0, 1, 2, 0, 2, 0, 1, 1]\n",
      "Step=4000 | Episodes=5 | AvgRew(last10)=0.20 | Eps=0.901 | Real Time Elapsed=1736538811.21s\n",
      "Last taken actions: [2, 0, 0, 2, 0, 1, 0, 2, 0, 2]\n",
      "Step=5000 | Episodes=7 | AvgRew(last10)=0.14 | Eps=0.876 | Real Time Elapsed=1736538861.02s\n",
      "Last taken actions: [0, 2, 0, 1, 0, 2, 1, 0, 1, 2]\n",
      "Step=6000 | Episodes=8 | AvgRew(last10)=0.12 | Eps=0.852 | Real Time Elapsed=1736538911.00s\n",
      "Last taken actions: [0, 0, 0, 1, 2, 1, 2, 2, 0, 2]\n",
      "Step=7000 | Episodes=10 | AvgRew(last10)=0.10 | Eps=0.827 | Real Time Elapsed=1736538961.55s\n",
      "Last taken actions: [1, 1, 0, 2, 1, 1, 0, 0, 2, 0]\n",
      "Step=8000 | Episodes=11 | AvgRew(last10)=0.10 | Eps=0.802 | Real Time Elapsed=1736539012.13s\n",
      "Last taken actions: [1, 1, 1, 2, 0, 0, 2, 0, 0, 1]\n",
      "Step=9000 | Episodes=13 | AvgRew(last10)=0.10 | Eps=0.777 | Real Time Elapsed=1736539060.90s\n",
      "Last taken actions: [2, 1, 1, 1, 1, 2, 1, 0, 2, 0]\n",
      "Step=10000 | Episodes=14 | AvgRew(last10)=0.20 | Eps=0.753 | Real Time Elapsed=1736539111.89s\n",
      "Last taken actions: [2, 0, 1, 1, 0, 0, 2, 0, 1, 0]\n",
      "Step=11000 | Episodes=16 | AvgRew(last10)=0.40 | Eps=0.728 | Real Time Elapsed=1736539162.17s\n",
      "Last taken actions: [1, 0, 2, 1, 0, 0, 0, 0, 1, 0]\n",
      "Step=12000 | Episodes=17 | AvgRew(last10)=0.50 | Eps=0.703 | Real Time Elapsed=1736539209.85s\n",
      "Last taken actions: [2, 2, 2, 2, 0, 0, 1, 0, 1, 1]\n",
      "Step=13000 | Episodes=19 | AvgRew(last10)=0.60 | Eps=0.678 | Real Time Elapsed=1736539260.91s\n",
      "Last taken actions: [2, 2, 2, 2, 0, 1, 0, 2, 0, 1]\n",
      "Step=14000 | Episodes=20 | AvgRew(last10)=0.60 | Eps=0.654 | Real Time Elapsed=1736539311.58s\n",
      "Last taken actions: [1, 1, 0, 0, 1, 1, 1, 2, 0, 2]\n",
      "Step=15000 | Episodes=22 | AvgRew(last10)=0.60 | Eps=0.629 | Real Time Elapsed=1736539361.01s\n",
      "Last taken actions: [1, 1, 2, 0, 0, 1, 1, 1, 1, 0]\n",
      "Step=16000 | Episodes=23 | AvgRew(last10)=0.60 | Eps=0.604 | Real Time Elapsed=1736539411.87s\n",
      "Last taken actions: [0, 0, 1, 1, 2, 1, 1, 2, 0, 2]\n",
      "Step=17000 | Episodes=25 | AvgRew(last10)=0.40 | Eps=0.579 | Real Time Elapsed=1736539461.54s\n",
      "Last taken actions: [1, 2, 0, 1, 0, 1, 1, 1, 1, 2]\n",
      "Step=18000 | Episodes=26 | AvgRew(last10)=0.50 | Eps=0.555 | Real Time Elapsed=1736539511.05s\n",
      "Last taken actions: [2, 0, 0, 0, 0, 0, 2, 2, 2, 1]\n",
      "Step=19000 | Episodes=28 | AvgRew(last10)=0.50 | Eps=0.530 | Real Time Elapsed=1736539562.32s\n",
      "Last taken actions: [2, 0, 1, 0, 1, 2, 0, 1, 2, 1]\n",
      "Step=20000 | Episodes=29 | AvgRew(last10)=0.60 | Eps=0.505 | Real Time Elapsed=1736539612.33s\n",
      "Last taken actions: [0, 1, 0, 1, 2, 1, 0, 0, 1, 2]\n",
      "Step=21000 | Episodes=30 | AvgRew(last10)=0.70 | Eps=0.480 | Real Time Elapsed=1736539662.26s\n",
      "Last taken actions: [2, 1, 0, 2, 1, 1, 1, 1, 1, 1]\n",
      "Step=22000 | Episodes=32 | AvgRew(last10)=0.80 | Eps=0.456 | Real Time Elapsed=1736539714.54s\n",
      "Last taken actions: [2, 1, 1, 0, 0, 2, 0, 2, 0, 1]\n",
      "Step=23000 | Episodes=33 | AvgRew(last10)=0.90 | Eps=0.431 | Real Time Elapsed=1736539763.09s\n",
      "Last taken actions: [0, 1, 1, 2, 0, 2, 0, 0, 0, 1]\n",
      "Step=24000 | Episodes=35 | AvgRew(last10)=0.90 | Eps=0.406 | Real Time Elapsed=1736539814.19s\n",
      "Last taken actions: [1, 2, 2, 2, 1, 2, 1, 1, 0, 1]\n",
      "Step=25000 | Episodes=36 | AvgRew(last10)=0.90 | Eps=0.381 | Real Time Elapsed=1736539864.30s\n",
      "Last taken actions: [1, 0, 0, 1, 1, 2, 2, 1, 2, 0]\n",
      "Step=26000 | Episodes=38 | AvgRew(last10)=1.00 | Eps=0.357 | Real Time Elapsed=1736539914.84s\n",
      "Last taken actions: [1, 0, 0, 0, 2, 2, 2, 2, 0, 1]\n",
      "Step=27000 | Episodes=39 | AvgRew(last10)=1.10 | Eps=0.332 | Real Time Elapsed=1736539962.49s\n",
      "Last taken actions: [1, 1, 1, 2, 2, 2, 2, 2, 2, 0]\n",
      "Step=28000 | Episodes=41 | AvgRew(last10)=1.10 | Eps=0.307 | Real Time Elapsed=1736540014.02s\n",
      "Last taken actions: [1, 2, 2, 2, 0, 0, 0, 0, 0, 0]\n",
      "Step=29000 | Episodes=42 | AvgRew(last10)=1.00 | Eps=0.282 | Real Time Elapsed=1736540065.24s\n",
      "Last taken actions: [0, 0, 1, 1, 1, 2, 1, 0, 1, 2]\n",
      "Step=30000 | Episodes=44 | AvgRew(last10)=1.00 | Eps=0.258 | Real Time Elapsed=1736540115.32s\n",
      "Last taken actions: [2, 1, 2, 2, 2, 2, 1, 1, 1, 0]\n",
      "Step=31000 | Episodes=45 | AvgRew(last10)=1.20 | Eps=0.233 | Real Time Elapsed=1736540165.77s\n",
      "Last taken actions: [1, 0, 2, 0, 1, 2, 1, 2, 2, 2]\n",
      "Step=32000 | Episodes=47 | AvgRew(last10)=1.10 | Eps=0.208 | Real Time Elapsed=1736540215.64s\n",
      "Last taken actions: [2, 2, 0, 2, 0, 0, 0, 1, 1, 1]\n",
      "Step=33000 | Episodes=48 | AvgRew(last10)=1.10 | Eps=0.183 | Real Time Elapsed=1736540266.59s\n",
      "Last taken actions: [1, 1, 1, 1, 1, 2, 2, 2, 2, 1]\n",
      "Step=34000 | Episodes=50 | AvgRew(last10)=0.80 | Eps=0.159 | Real Time Elapsed=1736540316.27s\n",
      "Last taken actions: [0, 0, 1, 0, 0, 0, 0, 0, 0, 2]\n",
      "Step=35000 | Episodes=51 | AvgRew(last10)=0.90 | Eps=0.134 | Real Time Elapsed=1736540365.29s\n",
      "Last taken actions: [1, 1, 0, 2, 2, 2, 2, 2, 2, 1]\n",
      "Step=36000 | Episodes=53 | AvgRew(last10)=0.90 | Eps=0.109 | Real Time Elapsed=1736540416.84s\n",
      "Last taken actions: [2, 2, 2, 2, 1, 0, 1, 0, 2, 0]\n",
      "Step=37000 | Episodes=54 | AvgRew(last10)=1.00 | Eps=0.084 | Real Time Elapsed=1736540465.89s\n",
      "Last taken actions: [1, 1, 2, 1, 2, 2, 1, 2, 0, 1]\n",
      "Step=38000 | Episodes=56 | AvgRew(last10)=0.90 | Eps=0.060 | Real Time Elapsed=1736540516.75s\n",
      "Last taken actions: [2, 2, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Step=39000 | Episodes=57 | AvgRew(last10)=1.20 | Eps=0.035 | Real Time Elapsed=1736540566.49s\n",
      "Last taken actions: [1, 2, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[INFO] Resetting agent 0 at step 40000\n",
      "Step=40000 | Episodes=58 | AvgRew(last10)=1.30 | Eps=0.010 | Real Time Elapsed=1736540616.95s\n",
      "Last taken actions: [1, 0, 1, 1, 1, 0, 2, 1, 2, 0]\n",
      "Step=41000 | Episodes=60 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736540666.80s\n",
      "Last taken actions: [2, 0, 2, 2, 2, 1, 0, 1, 0, 2]\n",
      "Step=42000 | Episodes=61 | AvgRew(last10)=1.30 | Eps=0.010 | Real Time Elapsed=1736540717.62s\n",
      "Last taken actions: [2, 2, 2, 1, 1, 0, 2, 2, 2, 2]\n",
      "Step=43000 | Episodes=63 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736540767.02s\n",
      "Last taken actions: [2, 0, 1, 0, 2, 2, 0, 0, 0, 0]\n",
      "Step=44000 | Episodes=64 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736540817.47s\n",
      "Last taken actions: [1, 2, 2, 0, 2, 0, 2, 1, 2, 1]\n",
      "Step=45000 | Episodes=66 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736540867.15s\n",
      "Last taken actions: [0, 0, 1, 1, 1, 1, 2, 2, 2, 1]\n",
      "Step=46000 | Episodes=67 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736540916.39s\n",
      "Last taken actions: [0, 0, 0, 2, 2, 2, 2, 1, 1, 2]\n",
      "Step=47000 | Episodes=69 | AvgRew(last10)=1.70 | Eps=0.010 | Real Time Elapsed=1736540966.63s\n",
      "Last taken actions: [1, 0, 1, 1, 0, 2, 2, 1, 2, 2]\n",
      "Step=48000 | Episodes=70 | AvgRew(last10)=1.60 | Eps=0.010 | Real Time Elapsed=1736541017.05s\n",
      "Last taken actions: [0, 1, 2, 1, 1, 1, 1, 1, 0, 1]\n",
      "Step=49000 | Episodes=72 | AvgRew(last10)=1.60 | Eps=0.010 | Real Time Elapsed=1736541067.35s\n",
      "Last taken actions: [0, 2, 2, 2, 2, 2, 1, 2, 0, 0]\n",
      "Step=50000 | Episodes=73 | AvgRew(last10)=1.80 | Eps=0.010 | Real Time Elapsed=1736541115.95s\n",
      "Last taken actions: [0, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n",
      "Step=51000 | Episodes=75 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736541165.30s\n",
      "Last taken actions: [1, 1, 2, 0, 0, 2, 2, 2, 1, 1]\n",
      "Step=52000 | Episodes=76 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736541216.28s\n",
      "Last taken actions: [1, 1, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "Step=53000 | Episodes=78 | AvgRew(last10)=1.80 | Eps=0.010 | Real Time Elapsed=1736541266.60s\n",
      "Last taken actions: [1, 1, 1, 1, 2, 1, 2, 1, 1, 1]\n",
      "Step=54000 | Episodes=79 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736541317.18s\n",
      "Last taken actions: [1, 2, 0, 0, 0, 1, 2, 2, 0, 1]\n",
      "Step=55000 | Episodes=81 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736541366.77s\n",
      "Last taken actions: [2, 2, 1, 2, 1, 1, 1, 1, 1, 1]\n",
      "Step=56000 | Episodes=82 | AvgRew(last10)=1.60 | Eps=0.010 | Real Time Elapsed=1736541418.53s\n",
      "Last taken actions: [1, 2, 1, 2, 2, 1, 2, 1, 0, 0]\n",
      "Step=57000 | Episodes=83 | AvgRew(last10)=1.50 | Eps=0.010 | Real Time Elapsed=1736541467.55s\n",
      "Last taken actions: [2, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "Step=58000 | Episodes=85 | AvgRew(last10)=1.90 | Eps=0.010 | Real Time Elapsed=1736541519.30s\n",
      "Last taken actions: [2, 1, 0, 0, 1, 1, 1, 1, 1, 2]\n",
      "Step=59000 | Episodes=86 | AvgRew(last10)=1.90 | Eps=0.010 | Real Time Elapsed=1736541567.78s\n",
      "Last taken actions: [1, 1, 1, 0, 0, 0, 0, 1, 1, 2]\n",
      "Step=60000 | Episodes=88 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736541619.36s\n",
      "Last taken actions: [1, 1, 0, 2, 2, 2, 0, 2, 2, 2]\n",
      "Step=61000 | Episodes=89 | AvgRew(last10)=1.10 | Eps=0.010 | Real Time Elapsed=1736541668.10s\n",
      "Last taken actions: [1, 2, 1, 1, 1, 2, 1, 2, 2, 2]\n",
      "Step=62000 | Episodes=91 | AvgRew(last10)=1.10 | Eps=0.010 | Real Time Elapsed=1736541718.65s\n",
      "Last taken actions: [0, 0, 2, 2, 1, 2, 2, 2, 1, 1]\n",
      "Step=63000 | Episodes=92 | AvgRew(last10)=1.00 | Eps=0.010 | Real Time Elapsed=1736541769.37s\n",
      "Last taken actions: [0, 2, 2, 0, 0, 1, 1, 0, 1, 1]\n",
      "Step=64000 | Episodes=94 | AvgRew(last10)=0.50 | Eps=0.010 | Real Time Elapsed=1736541819.48s\n",
      "Last taken actions: [1, 2, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "Step=65000 | Episodes=95 | AvgRew(last10)=0.60 | Eps=0.010 | Real Time Elapsed=1736541870.48s\n",
      "Last taken actions: [1, 0, 0, 1, 1, 1, 1, 2, 2, 0]\n",
      "Step=66000 | Episodes=97 | AvgRew(last10)=0.60 | Eps=0.010 | Real Time Elapsed=1736541920.08s\n",
      "Last taken actions: [2, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "Step=67000 | Episodes=98 | AvgRew(last10)=0.70 | Eps=0.010 | Real Time Elapsed=1736541970.06s\n",
      "Last taken actions: [2, 2, 0, 0, 1, 1, 1, 1, 2, 2]\n",
      "Step=68000 | Episodes=100 | AvgRew(last10)=0.80 | Eps=0.010 | Real Time Elapsed=1736542021.02s\n",
      "Last taken actions: [2, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Step=69000 | Episodes=101 | AvgRew(last10)=0.90 | Eps=0.010 | Real Time Elapsed=1736542070.48s\n",
      "Last taken actions: [2, 2, 2, 2, 2, 0, 0, 0, 0, 2]\n",
      "Step=70000 | Episodes=103 | AvgRew(last10)=1.00 | Eps=0.010 | Real Time Elapsed=1736542121.20s\n",
      "Last taken actions: [1, 2, 2, 2, 1, 1, 1, 0, 0, 2]\n",
      "Step=71000 | Episodes=104 | AvgRew(last10)=1.00 | Eps=0.010 | Real Time Elapsed=1736542170.21s\n",
      "Last taken actions: [1, 0, 1, 1, 0, 1, 2, 1, 1, 1]\n",
      "Step=72000 | Episodes=106 | AvgRew(last10)=1.00 | Eps=0.010 | Real Time Elapsed=1736542220.33s\n",
      "Last taken actions: [2, 2, 1, 2, 0, 0, 1, 1, 1, 1]\n",
      "Step=73000 | Episodes=107 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736542270.80s\n",
      "Last taken actions: [0, 1, 1, 1, 1, 2, 2, 2, 2, 0]\n",
      "Step=74000 | Episodes=108 | AvgRew(last10)=1.30 | Eps=0.010 | Real Time Elapsed=1736542320.19s\n",
      "Last taken actions: [2, 2, 2, 1, 0, 2, 2, 1, 1, 1]\n",
      "Step=75000 | Episodes=110 | AvgRew(last10)=1.30 | Eps=0.010 | Real Time Elapsed=1736542371.76s\n",
      "Last taken actions: [0, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
      "Step=76000 | Episodes=111 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736542421.45s\n",
      "Last taken actions: [2, 2, 2, 1, 2, 2, 2, 2, 2, 2]\n",
      "Step=77000 | Episodes=113 | AvgRew(last10)=1.60 | Eps=0.010 | Real Time Elapsed=1736542472.47s\n",
      "Last taken actions: [2, 2, 2, 1, 1, 2, 1, 1, 1, 0]\n",
      "Step=78000 | Episodes=114 | AvgRew(last10)=1.60 | Eps=0.010 | Real Time Elapsed=1736542522.13s\n",
      "Last taken actions: [2, 2, 2, 2, 2, 1, 1, 1, 1, 0]\n",
      "Step=79000 | Episodes=116 | AvgRew(last10)=2.00 | Eps=0.010 | Real Time Elapsed=1736542571.17s\n",
      "Last taken actions: [1, 1, 0, 1, 1, 2, 2, 0, 1, 1]\n",
      "[INFO] Resetting agent 1 at step 80000\n",
      "Step=80000 | Episodes=117 | AvgRew(last10)=2.30 | Eps=0.010 | Real Time Elapsed=1736542620.45s\n",
      "Last taken actions: [0, 1, 1, 0, 2, 0, 0, 0, 0, 0]\n",
      "Step=81000 | Episodes=119 | AvgRew(last10)=2.20 | Eps=0.010 | Real Time Elapsed=1736542671.25s\n",
      "Last taken actions: [0, 2, 2, 2, 1, 1, 1, 1, 1, 0]\n",
      "Step=82000 | Episodes=120 | AvgRew(last10)=2.00 | Eps=0.010 | Real Time Elapsed=1736542718.90s\n",
      "Last taken actions: [1, 1, 2, 2, 2, 2, 1, 1, 1, 1]\n",
      "Step=83000 | Episodes=122 | AvgRew(last10)=1.80 | Eps=0.010 | Real Time Elapsed=1736542771.15s\n",
      "Last taken actions: [1, 1, 2, 1, 0, 2, 0, 0, 0, 0]\n",
      "Step=84000 | Episodes=123 | AvgRew(last10)=1.40 | Eps=0.010 | Real Time Elapsed=1736542821.12s\n",
      "Last taken actions: [2, 2, 1, 1, 0, 0, 1, 1, 1, 2]\n",
      "Step=85000 | Episodes=125 | AvgRew(last10)=1.20 | Eps=0.010 | Real Time Elapsed=1736542870.25s\n",
      "Last taken actions: [1, 2, 2, 1, 2, 1, 1, 1, 2, 1]\n",
      "Step=86000 | Episodes=126 | AvgRew(last10)=0.90 | Eps=0.010 | Real Time Elapsed=1736542920.69s\n",
      "Last taken actions: [0, 1, 1, 1, 1, 2, 2, 2, 1, 1]\n",
      "Step=87000 | Episodes=128 | AvgRew(last10)=0.30 | Eps=0.010 | Real Time Elapsed=1736542972.55s\n",
      "Last taken actions: [1, 1, 1, 0, 0, 0, 0, 0, 2, 2]\n",
      "Step=88000 | Episodes=129 | AvgRew(last10)=0.40 | Eps=0.010 | Real Time Elapsed=1736543020.92s\n",
      "Last taken actions: [2, 2, 2, 2, 2, 2, 1, 1, 0, 0]\n",
      "Step=89000 | Episodes=131 | AvgRew(last10)=0.50 | Eps=0.010 | Real Time Elapsed=1736543070.59s\n",
      "Last taken actions: [0, 0, 0, 0, 2, 1, 1, 2, 1, 2]\n",
      "Step=90000 | Episodes=132 | AvgRew(last10)=0.50 | Eps=0.010 | Real Time Elapsed=1736543119.85s\n",
      "Last taken actions: [0, 0, 1, 2, 1, 1, 1, 2, 2, 1]\n",
      "Step=91000 | Episodes=133 | AvgRew(last10)=0.60 | Eps=0.010 | Real Time Elapsed=1736543170.42s\n",
      "Last taken actions: [1, 0, 0, 0, 0, 2, 2, 1, 1, 1]\n",
      "Step=92000 | Episodes=135 | AvgRew(last10)=0.70 | Eps=0.010 | Real Time Elapsed=1736543219.80s\n",
      "Last taken actions: [2, 2, 2, 1, 0, 2, 2, 2, 2, 1]\n",
      "Step=93000 | Episodes=136 | AvgRew(last10)=0.70 | Eps=0.010 | Real Time Elapsed=1736543268.03s\n",
      "Last taken actions: [1, 2, 2, 1, 1, 1, 0, 0, 0, 0]\n",
      "Step=94000 | Episodes=138 | AvgRew(last10)=0.80 | Eps=0.010 | Real Time Elapsed=1736543318.27s\n",
      "Last taken actions: [0, 2, 0, 0, 0, 1, 0, 1, 2, 0]\n",
      "Step=95000 | Episodes=139 | AvgRew(last10)=0.80 | Eps=0.010 | Real Time Elapsed=1736543369.27s\n",
      "Last taken actions: [1, 1, 0, 0, 0, 0, 2, 2, 1, 1]\n",
      "Step=96000 | Episodes=141 | AvgRew(last10)=0.70 | Eps=0.010 | Real Time Elapsed=1736543419.77s\n",
      "Last taken actions: [1, 1, 0, 2, 2, 2, 1, 0, 0, 0]\n",
      "Step=97000 | Episodes=142 | AvgRew(last10)=0.90 | Eps=0.010 | Real Time Elapsed=1736543467.44s\n",
      "Last taken actions: [0, 1, 2, 2, 2, 0, 1, 0, 0, 0]\n",
      "Step=98000 | Episodes=144 | AvgRew(last10)=0.90 | Eps=0.010 | Real Time Elapsed=1736543518.40s\n",
      "Last taken actions: [2, 1, 1, 0, 0, 1, 2, 0, 0, 0]\n",
      "Step=99000 | Episodes=145 | AvgRew(last10)=1.00 | Eps=0.010 | Real Time Elapsed=1736543570.24s\n",
      "Last taken actions: [0, 0, 0, 1, 1, 1, 1, 2, 2, 2]\n",
      "Step=100000 | Episodes=147 | AvgRew(last10)=0.80 | Eps=0.010 | Real Time Elapsed=1736543618.84s\n",
      "Last taken actions: [1, 1, 1, 0, 0, 0, 0, 2, 1, 1]\n",
      "Finished. Total elapsed time: 5009.13s\n",
      "Training done. Final 10-episode average reward: 0.8\n"
     ]
    }
   ],
   "source": [
    "env = make_vec_env_sb3(ENV_ID, N_ENVS)\n",
    "render_env = gym.make(ENV_ID, render_mode=\"human\")\n",
    "final_10 = run_training(env, render_env, verbose=1)\n",
    "print(\"Training done. Final 10-episode average reward:\", final_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use this line if you need to prematurely kill the render_env\n",
    "render_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
