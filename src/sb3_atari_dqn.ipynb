{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"atari_alien_rr4_plain_dqn_1\"\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor # required for minigrid\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed # may be required for seeded approaches\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from gymnasium.wrappers import FrameStackObservation, ClipReward\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reset weights\n",
    "def reset_weights(layer):\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "# Custom callback to reset weights during training\n",
    "class ResetWeightsCallback(BaseCallback):\n",
    "    def __init__(self, reset_interval, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.reset_interval = reset_interval  # Number of steps between resets\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Reset weights every reset_interval steps\n",
    "        if self.n_calls % self.reset_interval == 0: # n_calls inherited from BaseCallback\n",
    "            # if self.verbose > 0:\n",
    "            #     print(f\"Resetting weights at step {self.n_calls}...\")\n",
    "            print(f\"Policy weight reset at: {self.n_calls}\")\n",
    "            # Reset q_net and q_net_target\n",
    "            self.model.policy.q_net.apply(reset_weights)\n",
    "            self.model.policy.q_net_target.apply(reset_weights)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stack = 4 # run updates once every 4 frames (stack 4 environments for the model to train on)\n",
    "eval_freq = 5000 # once every 5000 timesteps, evaluate the model\n",
    "timesteps = 100000 # game timesteps\n",
    "replay_ratio = 4 # run gradient calculations 4 times per step\n",
    "reset_interval = 10000 # reset a part of the buffer at this timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs=n_stack) #seed can be used here\n",
    "env = VecFrameStack(env, n_stack= n_stack)\n",
    "eval_env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs= n_stack) #seed can be used here, different than env's seed\n",
    "eval_env = VecFrameStack(eval_env, n_stack= n_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f\"./logs/sb3_atari_dqn_1\"\n",
    "policy_kwargs = dict()\n",
    "# policy_kwargs.update(num_agent=1)\n",
    "# policy_kwargs.update(action_select_coef=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback frequencies are scaled to stack counts to match the given actual game timestep\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_path, log_path=log_path,\n",
    "                             eval_freq=max(eval_freq // n_stack, 1), deterministic=True,\n",
    "                             render=True)\n",
    "# Create and attach the callback\n",
    "reset_callback = ResetWeightsCallback(reset_interval=max(reset_interval // n_stack, 1), verbose=1)\n",
    "\n",
    "callback_list = CallbackList([eval_callback, reset_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./dqn_atari_logs/DQN_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x746ab3f300a0> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x746ab8504f70>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.889    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 860      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1172     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 848      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2140     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 136      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.53e+03 |\n",
      "|    ep_rew_mean      | 123      |\n",
      "|    exploration_rate | 0.736    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 585      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 2780     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0185   |\n",
      "|    n_updates        | 776      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.74e+03 |\n",
      "|    ep_rew_mean      | 145      |\n",
      "|    exploration_rate | 0.66     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 475      |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 3580     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 1576     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.62e+03 |\n",
      "|    ep_rew_mean      | 146      |\n",
      "|    exploration_rate | 0.564    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 420      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 4588     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00667  |\n",
      "|    n_updates        | 2584     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=276.00 +/- 44.09\n",
      "Episode length: 2443.80 +/- 158.75\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.44e+03 |\n",
      "|    mean_reward      | 276      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 2996     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.62e+03 |\n",
      "|    ep_rew_mean      | 146      |\n",
      "|    exploration_rate | 0.456    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 341      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 5728     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.62e+03 |\n",
      "|    ep_rew_mean      | 209      |\n",
      "|    exploration_rate | 0.35     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 6844     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0211   |\n",
      "|    n_updates        | 4840     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.62e+03 |\n",
      "|    ep_rew_mean      | 209      |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 325      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 7452     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 5448     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.69e+03 |\n",
      "|    ep_rew_mean      | 231      |\n",
      "|    exploration_rate | 0.207    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 321      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8348     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0303   |\n",
      "|    n_updates        | 6344     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.67e+03 |\n",
      "|    ep_rew_mean      | 246      |\n",
      "|    exploration_rate | 0.134    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 318      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9120     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0221   |\n",
      "|    n_updates        | 7116     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.74e+03 |\n",
      "|    ep_rew_mean      | 274      |\n",
      "|    exploration_rate | 0.0504   |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 314      |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 9996     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0173   |\n",
      "|    n_updates        | 7992     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=442.00 +/- 264.38\n",
      "Episode length: 2260.20 +/- 343.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.26e+03 |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    loss            | 0.0308   |\n",
      "|    n_updates       | 7996     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Policy weight reset at: 2500\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.68e+03 |\n",
      "|    ep_rew_mean      | 275      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 294      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 10984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 8980     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.65e+03 |\n",
      "|    ep_rew_mean      | 280      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 293      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 11832    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0258   |\n",
      "|    n_updates        | 9828     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.62e+03 |\n",
      "|    ep_rew_mean      | 293      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 293      |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 12640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 10636    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.58e+03 |\n",
      "|    ep_rew_mean      | 294      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 293      |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 13536    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0228   |\n",
      "|    n_updates        | 11532    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.63e+03 |\n",
      "|    ep_rew_mean      | 302      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 293      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 14136    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0165   |\n",
      "|    n_updates        | 12132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.56e+03 |\n",
      "|    ep_rew_mean      | 303      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 293      |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 14760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.029    |\n",
      "|    n_updates        | 12756    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=216.00 +/- 82.37\n",
      "Episode length: 2221.00 +/- 311.94\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.22e+03 |\n",
      "|    mean_reward      | 216      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 12996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.54e+03 |\n",
      "|    ep_rew_mean      | 300      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 275      |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 15608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0212   |\n",
      "|    n_updates        | 13604    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.54e+03 |\n",
      "|    ep_rew_mean      | 300      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 275      |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 16456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0199   |\n",
      "|    n_updates        | 14452    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.49e+03 |\n",
      "|    ep_rew_mean      | 303      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 275      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 17048    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 15044    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.5e+03  |\n",
      "|    ep_rew_mean      | 331      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 276      |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 18164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 16160    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.49e+03 |\n",
      "|    ep_rew_mean      | 330      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 277      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 19004    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.02     |\n",
      "|    n_updates        | 17000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.5e+03  |\n",
      "|    ep_rew_mean      | 331      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 277      |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 19408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0242   |\n",
      "|    n_updates        | 17404    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=362.00 +/- 102.25\n",
      "Episode length: 2373.80 +/- 505.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.37e+03 |\n",
      "|    mean_reward      | 362      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0277   |\n",
      "|    n_updates        | 17996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 5000\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.5e+03  |\n",
      "|    ep_rew_mean      | 333      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 20684    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0155   |\n",
      "|    n_updates        | 18680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.51e+03 |\n",
      "|    ep_rew_mean      | 332      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 21792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0216   |\n",
      "|    n_updates        | 19788    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.55e+03 |\n",
      "|    ep_rew_mean      | 358      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 22732    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 20728    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.55e+03 |\n",
      "|    ep_rew_mean      | 358      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 24076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 22072    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.55e+03 |\n",
      "|    ep_rew_mean      | 361      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 268      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 24720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 22716    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=250.00 +/- 137.55\n",
      "Episode length: 2323.00 +/- 647.93\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.32e+03 |\n",
      "|    mean_reward      | 250      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 22996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.55e+03 |\n",
      "|    ep_rew_mean      | 361      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 264      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 25856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0279   |\n",
      "|    n_updates        | 23852    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.53e+03 |\n",
      "|    ep_rew_mean      | 354      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 26716    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0202   |\n",
      "|    n_updates        | 24712    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.53e+03 |\n",
      "|    ep_rew_mean      | 354      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 27512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0285   |\n",
      "|    n_updates        | 25508    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.53e+03 |\n",
      "|    ep_rew_mean      | 353      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 28108    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0338   |\n",
      "|    n_updates        | 26104    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.51e+03 |\n",
      "|    ep_rew_mean      | 349      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 29028    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 27024    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.49e+03 |\n",
      "|    ep_rew_mean      | 346      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 29660    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 27656    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=276.00 +/- 168.36\n",
      "Episode length: 2149.40 +/- 609.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.15e+03 |\n",
      "|    mean_reward      | 276      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0374   |\n",
      "|    n_updates        | 27996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 7500\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 339      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 30984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0218   |\n",
      "|    n_updates        | 28980    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 339      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 31732    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0467   |\n",
      "|    n_updates        | 29728    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.46e+03 |\n",
      "|    ep_rew_mean      | 339      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 264      |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 32452    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0255   |\n",
      "|    n_updates        | 30448    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 342      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 264      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 33060    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0151   |\n",
      "|    n_updates        | 31056    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 342      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 34228    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 32224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=364.00 +/- 20.59\n",
      "Episode length: 2078.20 +/- 366.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.08e+03 |\n",
      "|    mean_reward      | 364      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0287   |\n",
      "|    n_updates        | 32996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 344      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 35320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 33316    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 344      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 35952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0297   |\n",
      "|    n_updates        | 33948    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 344      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 260      |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 36744    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.023    |\n",
      "|    n_updates        | 34740    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 347      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 261      |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 37576    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0392   |\n",
      "|    n_updates        | 35572    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 347      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 261      |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 38444    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0207   |\n",
      "|    n_updates        | 36440    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 347      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 262      |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 39272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0279   |\n",
      "|    n_updates        | 37268    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=526.00 +/- 278.32\n",
      "Episode length: 3055.00 +/- 432.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.06e+03 |\n",
      "|    mean_reward      | 526      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0319   |\n",
      "|    n_updates        | 37996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Policy weight reset at: 10000\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.48e+03 |\n",
      "|    ep_rew_mean      | 362      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 40072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0263   |\n",
      "|    n_updates        | 38068    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 364      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 40784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0496   |\n",
      "|    n_updates        | 38780    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 361      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 41324    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0289   |\n",
      "|    n_updates        | 39320    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 361      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 42096    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 40092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 360      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 43088    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0292   |\n",
      "|    n_updates        | 41084    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 358      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 169      |\n",
      "|    total_timesteps  | 43724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 41720    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 358      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 172      |\n",
      "|    total_timesteps  | 44432    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0327   |\n",
      "|    n_updates        | 42428    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=270.00 +/- 78.74\n",
      "Episode length: 2729.00 +/- 355.63\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.73e+03 |\n",
      "|    mean_reward      | 270      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 42996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 358      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 178      |\n",
      "|    total_timesteps  | 45376    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.025    |\n",
      "|    n_updates        | 43372    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 356      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 46300    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0317   |\n",
      "|    n_updates        | 44296    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 387      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 184      |\n",
      "|    total_timesteps  | 46908    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0259   |\n",
      "|    n_updates        | 44904    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 388      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total_timesteps  | 47960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0161   |\n",
      "|    n_updates        | 45956    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 388      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total_timesteps  | 48724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0145   |\n",
      "|    n_updates        | 46720    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 387      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total_timesteps  | 49756    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0439   |\n",
      "|    n_updates        | 47752    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=680.00 +/- 631.13\n",
      "Episode length: 2845.40 +/- 471.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.85e+03 |\n",
      "|    mean_reward      | 680      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 47996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Policy weight reset at: 12500\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 383      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 50784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0335   |\n",
      "|    n_updates        | 48780    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 383      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 51412    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0292   |\n",
      "|    n_updates        | 49408    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+03 |\n",
      "|    ep_rew_mean      | 389      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total_timesteps  | 52136    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0197   |\n",
      "|    n_updates        | 50132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 387      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 207      |\n",
      "|    total_timesteps  | 52992    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 50988    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 387      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total_timesteps  | 53484    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.036    |\n",
      "|    n_updates        | 51480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 384      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 211      |\n",
      "|    total_timesteps  | 54276    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0292   |\n",
      "|    n_updates        | 52272    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=286.00 +/- 146.91\n",
      "Episode length: 2545.40 +/- 455.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.55e+03 |\n",
      "|    mean_reward      | 286      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0276   |\n",
      "|    n_updates        | 52996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 384      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total_timesteps  | 55076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 53072    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 381      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total_timesteps  | 55896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 53892    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 56360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0222   |\n",
      "|    n_updates        | 54356    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 380      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total_timesteps  | 57376    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0353   |\n",
      "|    n_updates        | 55372    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 58248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.031    |\n",
      "|    n_updates        | 56244    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 381      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total_timesteps  | 58760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0275   |\n",
      "|    n_updates        | 56756    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total_timesteps  | 59628    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0384   |\n",
      "|    n_updates        | 57624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=344.00 +/- 77.10\n",
      "Episode length: 2241.40 +/- 191.35\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.24e+03 |\n",
      "|    mean_reward      | 344      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0247   |\n",
      "|    n_updates        | 57996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 15000\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 380      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 239      |\n",
      "|    total_timesteps  | 60724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0321   |\n",
      "|    n_updates        | 58720    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 61276    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0227   |\n",
      "|    n_updates        | 59272    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 62100    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0325   |\n",
      "|    n_updates        | 60096    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 248      |\n",
      "|    total_timesteps  | 63152    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0198   |\n",
      "|    n_updates        | 61148    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total_timesteps  | 63708    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0308   |\n",
      "|    n_updates        | 61704    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+03 |\n",
      "|    ep_rew_mean      | 380      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 253      |\n",
      "|    total_timesteps  | 64632    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.035    |\n",
      "|    n_updates        | 62628    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=282.00 +/- 158.16\n",
      "Episode length: 2253.00 +/- 426.04\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.25e+03 |\n",
      "|    mean_reward      | 282      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0237   |\n",
      "|    n_updates        | 62996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 377      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 259      |\n",
      "|    total_timesteps  | 65624    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0373   |\n",
      "|    n_updates        | 63620    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 375      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 66300    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0204   |\n",
      "|    n_updates        | 64296    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 264      |\n",
      "|    total_timesteps  | 67268    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0297   |\n",
      "|    n_updates        | 65264    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 68080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0205   |\n",
      "|    n_updates        | 66076    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.37e+03 |\n",
      "|    ep_rew_mean      | 385      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 270      |\n",
      "|    total_timesteps  | 68884    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 66880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.38e+03 |\n",
      "|    ep_rew_mean      | 390      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 69748    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0368   |\n",
      "|    n_updates        | 67744    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=404.00 +/- 138.65\n",
      "Episode length: 2741.00 +/- 761.98\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.74e+03 |\n",
      "|    mean_reward      | 404      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 67996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 17500\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.38e+03 |\n",
      "|    ep_rew_mean      | 391      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 278      |\n",
      "|    total_timesteps  | 70492    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0324   |\n",
      "|    n_updates        | 68488    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.37e+03 |\n",
      "|    ep_rew_mean      | 390      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 280      |\n",
      "|    total_timesteps  | 71068    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0567   |\n",
      "|    n_updates        | 69064    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.35e+03 |\n",
      "|    ep_rew_mean      | 388      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 282      |\n",
      "|    total_timesteps  | 71808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0294   |\n",
      "|    n_updates        | 69804    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.34e+03 |\n",
      "|    ep_rew_mean      | 386      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 285      |\n",
      "|    total_timesteps  | 72632    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0227   |\n",
      "|    n_updates        | 70628    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 384      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 287      |\n",
      "|    total_timesteps  | 73164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.044    |\n",
      "|    n_updates        | 71160    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.33e+03 |\n",
      "|    ep_rew_mean      | 384      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 288      |\n",
      "|    total_timesteps  | 73616    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0371   |\n",
      "|    n_updates        | 71612    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.33e+03 |\n",
      "|    ep_rew_mean      | 384      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 292      |\n",
      "|    total_timesteps  | 74676    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 72672    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=254.00 +/- 92.43\n",
      "Episode length: 2277.80 +/- 164.03\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.28e+03 |\n",
      "|    mean_reward      | 254      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 72996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 75680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0375   |\n",
      "|    n_updates        | 73676    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.33e+03 |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 300      |\n",
      "|    total_timesteps  | 76336    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0232   |\n",
      "|    n_updates        | 74332    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 383      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 303      |\n",
      "|    total_timesteps  | 77220    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.05     |\n",
      "|    n_updates        | 75216    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 384      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 77684    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0229   |\n",
      "|    n_updates        | 75680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.31e+03 |\n",
      "|    ep_rew_mean      | 385      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 308      |\n",
      "|    total_timesteps  | 78624    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0197   |\n",
      "|    n_updates        | 76620    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.31e+03 |\n",
      "|    ep_rew_mean      | 385      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 310      |\n",
      "|    total_timesteps  | 79220    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0337   |\n",
      "|    n_updates        | 77216    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=302.00 +/- 76.52\n",
      "Episode length: 2952.60 +/- 501.02\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.95e+03 |\n",
      "|    mean_reward      | 302      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0207   |\n",
      "|    n_updates        | 77996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 20000\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.31e+03 |\n",
      "|    ep_rew_mean      | 385      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 318      |\n",
      "|    total_timesteps  | 80372    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.022    |\n",
      "|    n_updates        | 78368    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.31e+03 |\n",
      "|    ep_rew_mean      | 385      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 320      |\n",
      "|    total_timesteps  | 80936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0289   |\n",
      "|    n_updates        | 78932    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 383      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 322      |\n",
      "|    total_timesteps  | 81704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 79700    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.33e+03 |\n",
      "|    ep_rew_mean      | 374      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 324      |\n",
      "|    total_timesteps  | 82232    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0232   |\n",
      "|    n_updates        | 80228    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.33e+03 |\n",
      "|    ep_rew_mean      | 374      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 326      |\n",
      "|    total_timesteps  | 82700    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 80696    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 374      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 329      |\n",
      "|    total_timesteps  | 83672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0386   |\n",
      "|    n_updates        | 81668    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.3e+03  |\n",
      "|    ep_rew_mean      | 373      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 332      |\n",
      "|    total_timesteps  | 84540    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.042    |\n",
      "|    n_updates        | 82536    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=266.00 +/- 70.03\n",
      "Episode length: 2757.80 +/- 431.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.76e+03 |\n",
      "|    mean_reward      | 266      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 82996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.3e+03  |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 340      |\n",
      "|    total_timesteps  | 85404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0439   |\n",
      "|    n_updates        | 83400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.3e+03  |\n",
      "|    ep_rew_mean      | 364      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 343      |\n",
      "|    total_timesteps  | 86196    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0283   |\n",
      "|    n_updates        | 84192    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.29e+03 |\n",
      "|    ep_rew_mean      | 364      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 346      |\n",
      "|    total_timesteps  | 86936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0316   |\n",
      "|    n_updates        | 84932    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.29e+03 |\n",
      "|    ep_rew_mean      | 365      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 348      |\n",
      "|    total_timesteps  | 87572    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0248   |\n",
      "|    n_updates        | 85568    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.28e+03 |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 349      |\n",
      "|    total_timesteps  | 87964    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0205   |\n",
      "|    n_updates        | 85960    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.28e+03 |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 352      |\n",
      "|    total_timesteps  | 88812    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 86808    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.27e+03 |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 355      |\n",
      "|    total_timesteps  | 89692    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0255   |\n",
      "|    n_updates        | 87688    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=666.00 +/- 678.57\n",
      "Episode length: 2393.40 +/- 345.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.39e+03 |\n",
      "|    mean_reward      | 666      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 87996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 22500\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.25e+03 |\n",
      "|    ep_rew_mean      | 365      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 361      |\n",
      "|    total_timesteps  | 90588    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0225   |\n",
      "|    n_updates        | 88584    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.25e+03 |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 364      |\n",
      "|    total_timesteps  | 91456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0337   |\n",
      "|    n_updates        | 89452    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.26e+03 |\n",
      "|    ep_rew_mean      | 367      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 366      |\n",
      "|    total_timesteps  | 91944    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0361   |\n",
      "|    n_updates        | 89940    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.26e+03 |\n",
      "|    ep_rew_mean      | 367      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 368      |\n",
      "|    total_timesteps  | 92548    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0267   |\n",
      "|    n_updates        | 90544    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.26e+03 |\n",
      "|    ep_rew_mean      | 367      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 369      |\n",
      "|    total_timesteps  | 92928    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0225   |\n",
      "|    n_updates        | 90924    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 365      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 372      |\n",
      "|    total_timesteps  | 93704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0301   |\n",
      "|    n_updates        | 91700    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 375      |\n",
      "|    total_timesteps  | 94576    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0258   |\n",
      "|    n_updates        | 92572    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=370.00 +/- 94.87\n",
      "Episode length: 1966.20 +/- 87.05\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.97e+03 |\n",
      "|    mean_reward      | 370      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0235   |\n",
      "|    n_updates        | 92996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.23e+03 |\n",
      "|    ep_rew_mean      | 367      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 380      |\n",
      "|    total_timesteps  | 95136    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0294   |\n",
      "|    n_updates        | 93132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.23e+03 |\n",
      "|    ep_rew_mean      | 378      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 382      |\n",
      "|    total_timesteps  | 95676    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0399   |\n",
      "|    n_updates        | 93672    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.23e+03 |\n",
      "|    ep_rew_mean      | 378      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 385      |\n",
      "|    total_timesteps  | 96556    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0211   |\n",
      "|    n_updates        | 94552    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.23e+03 |\n",
      "|    ep_rew_mean      | 378      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 386      |\n",
      "|    total_timesteps  | 97092    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 95088    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.22e+03 |\n",
      "|    ep_rew_mean      | 377      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 389      |\n",
      "|    total_timesteps  | 97932    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 95928    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.22e+03 |\n",
      "|    ep_rew_mean      | 377      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 392      |\n",
      "|    total_timesteps  | 98664    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 96660    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.21e+03 |\n",
      "|    ep_rew_mean      | 369      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 394      |\n",
      "|    total_timesteps  | 99276    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0188   |\n",
      "|    n_updates        | 97272    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.2e+03  |\n",
      "|    ep_rew_mean      | 366      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 396      |\n",
      "|    total_timesteps  | 99888    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0201   |\n",
      "|    n_updates        | 97884    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=350.00 +/- 49.80\n",
      "Episode length: 2519.00 +/- 284.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.52e+03 |\n",
      "|    mean_reward      | 350      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0377   |\n",
      "|    n_updates        | 97996    |\n",
      "----------------------------------\n",
      "Policy weight reset at: 25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x746ab3f339a0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\n",
    "    policy= \"CnnPolicy\", \n",
    "    env= env, \n",
    "    verbose= 1, \n",
    "    buffer_size= timesteps,\n",
    "    learning_starts= 2000,\n",
    "    tau= 0.005,\n",
    "    train_freq= (1, \"step\"),\n",
    "    gradient_steps= replay_ratio,\n",
    "    target_update_interval= 1,\n",
    "    policy_kwargs= policy_kwargs,\n",
    "    tensorboard_log=\"./dqn_atari_logs\",\n",
    "    )\n",
    "# need reset, reset_frequency and all_reset\n",
    "model.learn(\n",
    "    total_timesteps=timesteps,\n",
    "    callback=callback_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./models/{FNAME}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:157: UserWarning: The number of environments used is greater than the target network update interval (4 > 1), therefore the target network will be updated after each call to env.step() which corresponds to 4 steps.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(f\"./models/{FNAME}\", env= env) # use this cell if you already have a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward: 140.0, std_reward:0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "eval_eps = 10\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=eval_eps)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(eval_eps):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "\n",
    "print(f\"mean_reward: {mean_reward}, std_reward:{std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
