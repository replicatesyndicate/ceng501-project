{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"atari_empty_16x16_plain_dqn_1\"\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from gymnasium.wrappers import FrameStackObservation, ClipReward\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs=4) #seed can be used here\n",
    "env = VecFrameStack(env, n_stack= 4)\n",
    "eval_env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs=4) #seed can be used here, different than env's seed\n",
    "eval_env = VecFrameStack(eval_env, n_stack= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f\"./logs/sb3_atari_dqn_1\"\n",
    "policy_kwargs = dict()\n",
    "# policy_kwargs.update(num_agent=1)\n",
    "# policy_kwargs.update(action_select_coef=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(env, best_model_save_path=log_path, log_path=log_path,\n",
    "                             eval_freq=max(5000 // 4, 1), deterministic=True,\n",
    "                             render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 20000\n",
    "replay_ratio = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:157: UserWarning: The number of environments used is greater than the target network update interval (4 > 1), therefore the target network will be updated after each call to env.step() which corresponds to 4 steps.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_atari_logs/DQN_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x749e94754790> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x749e9b904fa0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.4      |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 873      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1264     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 805      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2316     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0424   |\n",
      "|    n_updates        | 78       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.7e+03  |\n",
      "|    ep_rew_mean      | 245      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 735      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 3072     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 267      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.7e+03  |\n",
      "|    ep_rew_mean      | 245      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 702      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 3808     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0303   |\n",
      "|    n_updates        | 451      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.45e+03 |\n",
      "|    ep_rew_mean      | 263      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 679      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4624     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 655      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=226.00 +/- 106.70\n",
      "Episode length: 2204.20 +/- 704.42\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.2e+03  |\n",
      "|    mean_reward      | 226      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0308   |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.45e+03 |\n",
      "|    ep_rew_mean      | 263      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 535      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 5560     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 889      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.34e+03 |\n",
      "|    ep_rew_mean      | 252      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 539      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 6392     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.016    |\n",
      "|    n_updates        | 1097     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.31e+03 |\n",
      "|    ep_rew_mean      | 289      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 543      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 7144     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0155   |\n",
      "|    n_updates        | 1285     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.28e+03 |\n",
      "|    ep_rew_mean      | 287      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 548      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 7952     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.031    |\n",
      "|    n_updates        | 1487     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.26e+03 |\n",
      "|    ep_rew_mean      | 294      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 553      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 8892     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0273   |\n",
      "|    n_updates        | 1722     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.32e+03 |\n",
      "|    ep_rew_mean      | 316      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 554      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 9428     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0013   |\n",
      "|    n_updates        | 1856     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.29e+03 |\n",
      "|    ep_rew_mean      | 310      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 556      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 9968     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0316   |\n",
      "|    n_updates        | 1991     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=270.00 +/- 46.48\n",
      "Episode length: 2117.80 +/- 355.28\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.12e+03 |\n",
      "|    mean_reward      | 270      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0298   |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 304      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 468      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 10672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00211  |\n",
      "|    n_updates        | 2167     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 305      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 474      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 11316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0325   |\n",
      "|    n_updates        | 2328     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 314      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 481      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 12164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0416   |\n",
      "|    n_updates        | 2540     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 338      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 484      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 12676    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0274   |\n",
      "|    n_updates        | 2668     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | 338      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 488      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 13272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.023    |\n",
      "|    n_updates        | 2817     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | 343      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 491      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 13896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0658   |\n",
      "|    n_updates        | 2973     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | 345      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 494      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 14416    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0304   |\n",
      "|    n_updates        | 3103     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=318.00 +/- 92.39\n",
      "Episode length: 3006.20 +/- 247.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.01e+03 |\n",
      "|    mean_reward      | 318      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0138   |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | 348      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 434      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 15148    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0442   |\n",
      "|    n_updates        | 3286     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.27e+03 |\n",
      "|    ep_rew_mean      | 338      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 441      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 16172    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0163   |\n",
      "|    n_updates        | 3542     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.29e+03 |\n",
      "|    ep_rew_mean      | 340      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 444      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 16600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0537   |\n",
      "|    n_updates        | 3649     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.28e+03 |\n",
      "|    ep_rew_mean      | 341      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 448      |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 17332    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 3832     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.27e+03 |\n",
      "|    ep_rew_mean      | 340      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 450      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 17836    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0499   |\n",
      "|    n_updates        | 3958     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.27e+03 |\n",
      "|    ep_rew_mean      | 338      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 454      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 18592    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0569   |\n",
      "|    n_updates        | 4147     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.26e+03 |\n",
      "|    ep_rew_mean      | 340      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 457      |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 19288    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0208   |\n",
      "|    n_updates        | 4321     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=374.00 +/- 139.80\n",
      "Episode length: 2253.80 +/- 494.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.25e+03 |\n",
      "|    mean_reward      | 374      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x749e947c0610>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\n",
    "    policy= \"CnnPolicy\", \n",
    "    env= env, \n",
    "    verbose= 1, \n",
    "    buffer_size= timesteps,\n",
    "    learning_starts= 2000,\n",
    "    tau= 0.005,\n",
    "    train_freq= (1, \"step\"),\n",
    "    gradient_steps= replay_ratio,\n",
    "    target_update_interval= 1,\n",
    "    policy_kwargs= policy_kwargs,\n",
    "    tensorboard_log=\"./dqn_atari_logs\",\n",
    "    )\n",
    "# need reset, reset_frequency and all_reset\n",
    "model.learn(\n",
    "    total_timesteps=timesteps,\n",
    "    callback=eval_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./models/{FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward: 302.0, std_reward:59.9666574022598\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "\n",
    "print(f\"mean_reward: {mean_reward}, std_reward:{std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
