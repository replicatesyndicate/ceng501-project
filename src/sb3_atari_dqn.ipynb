{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"atari_empty_16x16_plain_dqn_3\"\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from gymnasium.wrappers import FrameStackObservation, ClipReward\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reset weights\n",
    "def reset_weights(layer):\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "# Custom callback to reset weights during training\n",
    "class ResetWeightsCallback(BaseCallback):\n",
    "    def __init__(self, reset_interval, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.reset_interval = reset_interval  # Number of steps between resets\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Reset weights every reset_interval steps\n",
    "        if self.n_calls % self.reset_interval == 0: # n_calls inherited from BaseCallback\n",
    "            # if self.verbose > 0:\n",
    "            #     print(f\"Resetting weights at step {self.n_calls}...\")\n",
    "            print(f\"Policy weight reset at: {self.n_calls}\")\n",
    "            # Reset q_net and q_net_target\n",
    "            self.model.policy.q_net.apply(reset_weights)\n",
    "            self.model.policy.q_net_target.apply(reset_weights)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stack = 4 # run updates once every 4 frames (stack 4 frames for the model)\n",
    "eval_freq = 5000 # once every 5000 timesteps, evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs=n_stack) #seed can be used here\n",
    "env = VecFrameStack(env, n_stack= n_stack)\n",
    "eval_env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs= n_stack) #seed can be used here, different than env's seed\n",
    "eval_env = VecFrameStack(eval_env, n_stack= n_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f\"./logs/sb3_atari_dqn_1\"\n",
    "policy_kwargs = dict()\n",
    "# policy_kwargs.update(num_agent=1)\n",
    "# policy_kwargs.update(action_select_coef=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100000\n",
    "replay_ratio = 1\n",
    "reset_interval = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(env, best_model_save_path=log_path, log_path=log_path,\n",
    "                             eval_freq=max(eval_freq // n_stack, 1), deterministic=True,\n",
    "                             render=True)\n",
    "# Create and attach the callback\n",
    "reset_callback = ResetWeightsCallback(reset_interval=reset_interval // n_stack, verbose=1)\n",
    "\n",
    "callback_list = CallbackList([eval_callback, reset_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 5.65GB > 0.51GB\n",
      "  warnings.warn(\n",
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:157: UserWarning: The number of environments used is greater than the target network update interval (4 > 1), therefore the target network will be updated after each call to env.step() which corresponds to 4 steps.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_atari_logs/DQN_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x75ba2c22fb80> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x75ba2c1c3df0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.913    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 782      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 916      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.804    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 930      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2064     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 15       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.54e+03 |\n",
      "|    ep_rew_mean      | 200      |\n",
      "|    exploration_rate | 0.729    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2852     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00117  |\n",
      "|    n_updates        | 212      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.54e+03 |\n",
      "|    ep_rew_mean      | 200      |\n",
      "|    exploration_rate | 0.628    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 729      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 3916     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 478      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=304.00 +/- 271.12\n",
      "Episode length: 2714.20 +/- 670.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.71e+03 |\n",
      "|    mean_reward      | 304      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00393  |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.54e+03 |\n",
      "|    ep_rew_mean      | 190      |\n",
      "|    exploration_rate | 0.488    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 474      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 5392     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0474   |\n",
      "|    n_updates        | 847      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.423    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 480      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 6076     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0013   |\n",
      "|    n_updates        | 1018     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.324    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 489      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 7112     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0298   |\n",
      "|    n_updates        | 1277     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 216      |\n",
      "|    exploration_rate | 0.21     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 495      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 8312     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00128  |\n",
      "|    n_updates        | 1577     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.5e+03  |\n",
      "|    ep_rew_mean      | 236      |\n",
      "|    exploration_rate | 0.141    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 495      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 9040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000467 |\n",
      "|    n_updates        | 1759     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.55e+03 |\n",
      "|    ep_rew_mean      | 249      |\n",
      "|    exploration_rate | 0.0553   |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 9944     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000649 |\n",
      "|    n_updates        | 1985     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=208.00 +/- 234.21\n",
      "Episode length: 2312.60 +/- 566.52\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.31e+03 |\n",
      "|    mean_reward      | 208      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0504   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0513   |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.52e+03 |\n",
      "|    ep_rew_mean      | 231      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 11108    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0375   |\n",
      "|    n_updates        | 2276     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.55e+03 |\n",
      "|    ep_rew_mean      | 244      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 275      |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 12072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.017    |\n",
      "|    n_updates        | 2517     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.56e+03 |\n",
      "|    ep_rew_mean      | 255      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 281      |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 12768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0302   |\n",
      "|    n_updates        | 2691     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.57e+03 |\n",
      "|    ep_rew_mean      | 288      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 287      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 13672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0116   |\n",
      "|    n_updates        | 2917     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.57e+03 |\n",
      "|    ep_rew_mean      | 288      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 290      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 14192    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0435   |\n",
      "|    n_updates        | 3047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.46e+03 |\n",
      "|    ep_rew_mean      | 283      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 208      |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 14532    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0297   |\n",
      "|    n_updates        | 3132     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 283      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 212      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 14988    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0567   |\n",
      "|    n_updates        | 3246     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=252.00 +/- 79.35\n",
      "Episode length: 2353.80 +/- 288.75\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.35e+03 |\n",
      "|    mean_reward      | 252      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0783   |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 273      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 15672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00162  |\n",
      "|    n_updates        | 3417     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 273      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 213      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 16472    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0185   |\n",
      "|    n_updates        | 3617     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.46e+03 |\n",
      "|    ep_rew_mean      | 285      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 219      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 17204    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 3800     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 296      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 226      |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 18076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0317   |\n",
      "|    n_updates        | 4018     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 296      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 229      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 18520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00528  |\n",
      "|    n_updates        | 4129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.44e+03 |\n",
      "|    ep_rew_mean      | 300      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 19544    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0507   |\n",
      "|    n_updates        | 4385     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=240.00 +/- 69.86\n",
      "Episode length: 2145.40 +/- 309.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.15e+03 |\n",
      "|    mean_reward      | 240      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0416   |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.45e+03 |\n",
      "|    ep_rew_mean      | 305      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 20188    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0521   |\n",
      "|    n_updates        | 4546     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+03 |\n",
      "|    ep_rew_mean      | 299      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 239      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 20888    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0314   |\n",
      "|    n_updates        | 4721     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.4e+03  |\n",
      "|    ep_rew_mean      | 297      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 243      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 21476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0628   |\n",
      "|    n_updates        | 4868     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+03 |\n",
      "|    ep_rew_mean      | 297      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 22396    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00926  |\n",
      "|    n_updates        | 5098     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "    policy= \"CnnPolicy\", \n",
    "    env= env, \n",
    "    verbose= 1, \n",
    "    buffer_size= timesteps,\n",
    "    learning_starts= 2000,\n",
    "    tau= 0.005,\n",
    "    train_freq= (1, \"step\"),\n",
    "    gradient_steps= replay_ratio,\n",
    "    target_update_interval= 1,\n",
    "    policy_kwargs= policy_kwargs,\n",
    "    tensorboard_log=\"./dqn_atari_logs\",\n",
    "    )\n",
    "# need reset, reset_frequency and all_reset\n",
    "model.learn(\n",
    "    total_timesteps=timesteps,\n",
    "    callback=eval_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./models/{FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward: 522.0, std_reward:247.37825288412077\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "\n",
    "print(f\"mean_reward: {mean_reward}, std_reward:{std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
