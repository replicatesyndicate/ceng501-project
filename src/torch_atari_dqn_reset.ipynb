{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"atari_empty_16x16_reset_dqn_1\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stack = 4\n",
    "eval_freq = 5000 # once every 5000 timesteps, evaluate the model\n",
    "\n",
    "reset_frequency = 40000 # once every 40000 timesteps, reset a part of the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.index] = (state, action, reward, next_state, done)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in batch:\n",
    "            state, action, reward, next_state, done = self.buffer[i]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return (\n",
    "            torch.tensor(np.array(states)).float(),\n",
    "            torch.tensor(np.array(actions)).long(),\n",
    "            torch.tensor(np.array(rewards)).unsqueeze(1).float(),\n",
    "            torch.tensor(np.array(next_states)).float(),\n",
    "            torch.tensor(np.array(dones)).unsqueeze(1).int()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Define the Double DQN agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size, seed, learning_rate=1e-3, capacity=1000000,\n",
    "                 discount_factor=0.99, tau=1e-3, update_every=4, batch_size=64):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = seed\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(capacity)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay buffer\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every update_every steps\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_every == 0:\n",
    "            if len(self.replay_buffer) > self.batch_size:\n",
    "                experiences = self.replay_buffer.sample(self.batch_size)\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def act(self, state, eps=0.0):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + self.discount_factor * (Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions.view(-1, 1))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # Update target network parameters with polyak averaging\n",
    "        for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make_vec(\"AlienNoFrameskip-v4\", num_envs=n_stack) #seed can be used here\n",
    "eval_env = gym.make_vec(\"AlienNoFrameskip-v4\", num_envs=n_stack) #seed can be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f\"./logs/torch_atari_reset_dqn_1\"\n",
    "policy_kwargs = dict()\n",
    "# policy_kwargs.update(num_agent=1)\n",
    "# policy_kwargs.update(action_select_coef=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Create the DDQN agent\n",
    "agent = DDQNAgent(state_size, action_size)\n",
    "\n",
    "# Set the number of episodes and the maximum number of steps per episode\n",
    "num_episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "# Set the exploration rate\n",
    "eps = eps_start = 1.0\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "\n",
    "# Set the rewards and scores lists\n",
    "rewards = []\n",
    "scores = []\n",
    "\n",
    "# Run the training loop\n",
    "for i_episode in range(num_episodes):\n",
    "    print(f'Episode: {i_episode}')\n",
    "    # Initialize the environment and the state\n",
    "    state = env.reset()[0]\n",
    "    score = 0\n",
    "    # eps = eps_end + (eps_start - eps_end) * np.exp(-i_episode / eps_decay)\n",
    "    # Update the exploration rate\n",
    "    eps = max(eps_end, eps_decay * eps)\n",
    "    \n",
    "    # Run the episode\n",
    "    for t in range(max_steps):\n",
    "        # Select an action and take a step in the environment\n",
    "        action = agent.act(state, eps)\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        # Store the experience in the replay buffer and learn from it\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        # Update the state and the score\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # Break the loop if the episode is done or truncated\n",
    "        if done or trunc:\n",
    "            break\n",
    "        \n",
    "    print(f\"\\tScore: {score}, Epsilon: {eps}\")\n",
    "    # Save the rewards and scores\n",
    "    rewards.append(score)\n",
    "    scores.append(np.mean(rewards[-100:]))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(env, best_model_save_path=log_path, log_path=log_path,\n",
    "                             eval_freq=max(eval_freq // n_stack, 1), deterministic=True,\n",
    "                             render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 20000\n",
    "replay_ratio = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:157: UserWarning: The number of environments used is greater than the target network update interval (4 > 1), therefore the target network will be updated after each call to env.step() which corresponds to 4 steps.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_atari_logs/DQN_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobot/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x79c51dd06f20> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x79c51dd06ec0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.293    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 889      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1488     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 752      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2264     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 65       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.74e+03 |\n",
      "|    ep_rew_mean      | 268      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 698      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 2988     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000986 |\n",
      "|    n_updates        | 246      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.74e+03 |\n",
      "|    ep_rew_mean      | 268      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 3900     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0576   |\n",
      "|    n_updates        | 474      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.74e+03 |\n",
      "|    ep_rew_mean      | 268      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 654      |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 4728     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.019    |\n",
      "|    n_updates        | 681      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=340.00 +/- 131.61\n",
      "Episode length: 2605.40 +/- 417.65\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.61e+03 |\n",
      "|    mean_reward      | 340      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0151   |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.59e+03 |\n",
      "|    ep_rew_mean      | 234      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 510      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 5956     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0309   |\n",
      "|    n_updates        | 988      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.59e+03 |\n",
      "|    ep_rew_mean      | 234      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 519      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 6760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000977 |\n",
      "|    n_updates        | 1189     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.48e+03 |\n",
      "|    ep_rew_mean      | 292      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 524      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 7308     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0143   |\n",
      "|    n_updates        | 1326     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.52e+03 |\n",
      "|    ep_rew_mean      | 314      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 528      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 8200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0288   |\n",
      "|    n_updates        | 1549     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.52e+03 |\n",
      "|    ep_rew_mean      | 314      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 536      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 9032     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0299   |\n",
      "|    n_updates        | 1757     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 318      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 539      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 9564     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00662  |\n",
      "|    n_updates        | 1890     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=258.00 +/- 70.26\n",
      "Episode length: 2516.20 +/- 481.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.52e+03 |\n",
      "|    mean_reward      | 258      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0289   |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 302      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 464      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 10356    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0416   |\n",
      "|    n_updates        | 2088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+03 |\n",
      "|    ep_rew_mean      | 303      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 473      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 11264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0364   |\n",
      "|    n_updates        | 2315     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 319      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 479      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 12016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0166   |\n",
      "|    n_updates        | 2503     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 318      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 488      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 13160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0321   |\n",
      "|    n_updates        | 2789     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 319      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 493      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 13828    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.052    |\n",
      "|    n_updates        | 2956     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 333      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 495      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 14288    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 3071     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=234.00 +/- 54.63\n",
      "Episode length: 2114.60 +/- 298.21\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.11e+03 |\n",
      "|    mean_reward      | 234      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0184   |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 333      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 463      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 15368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0445   |\n",
      "|    n_updates        | 3341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+03 |\n",
      "|    ep_rew_mean      | 333      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 469      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 16264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0318   |\n",
      "|    n_updates        | 3565     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | 355      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 477      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 17312    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0334   |\n",
      "|    n_updates        | 3827     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.5e+03  |\n",
      "|    ep_rew_mean      | 380      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 481      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 17960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0674   |\n",
      "|    n_updates        | 3989     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.46e+03 |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 485      |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 18724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0143   |\n",
      "|    n_updates        | 4180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.43e+03 |\n",
      "|    ep_rew_mean      | 376      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 488      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 19188    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0398   |\n",
      "|    n_updates        | 4296     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+03 |\n",
      "|    ep_rew_mean      | 377      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 491      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 19952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0178   |\n",
      "|    n_updates        | 4487     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=214.00 +/- 47.58\n",
      "Episode length: 1843.40 +/- 118.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.84e+03 |\n",
      "|    mean_reward      | 214      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0605   |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x79c51dd06530>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\n",
    "    policy= \"CnnPolicy\", \n",
    "    env= env, \n",
    "    verbose= 1, \n",
    "    buffer_size= timesteps,\n",
    "    learning_starts= 2000,\n",
    "    tau= 0.005,\n",
    "    train_freq= (1, \"step\"),\n",
    "    gradient_steps= replay_ratio,\n",
    "    target_update_interval= 1,\n",
    "    policy_kwargs= policy_kwargs,\n",
    "    tensorboard_log=\"./dqn_atari_logs\",\n",
    "    )\n",
    "# need reset, reset_frequency and all_reset\n",
    "model.learn(\n",
    "    total_timesteps=timesteps,\n",
    "    callback=eval_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./models/{FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward: 302.0, std_reward:59.9666574022598\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "\n",
    "print(f\"mean_reward: {mean_reward}, std_reward:{std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
